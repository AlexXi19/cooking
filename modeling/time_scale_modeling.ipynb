{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f399f969",
      "metadata": {
        "id": "f399f969"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "842c0fc9",
      "metadata": {
        "id": "842c0fc9"
      },
      "source": [
        "### Train & Val Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "61ced19f",
      "metadata": {
        "id": "61ced19f"
      },
      "outputs": [],
      "source": [
        "tensor = np.load(\"embedding_new.npy\")\n",
        "# tensor = np.load(\"onehot.npy\")\n",
        "y = pd.read_csv(\"label.csv\")[\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "1ef96c50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ef96c50",
        "outputId": "f0462a2b-55aa-44a5-fabc-83efef78be65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "y -= 1\n",
        "y.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "7bf84c3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bf84c3e",
        "outputId": "fcf8175d-7204-473a-ab29-813fca27e174"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(170, 75, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "X_train, y_train = tensor[:170], y[:170]\n",
        "X_val, y_val = tensor[170:190], y[170:190]\n",
        "X_test, y_test = tensor[190:], y[190:]\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "LLzZtbFlHr3R",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLzZtbFlHr3R",
        "outputId": "6bacd303-4678-41d6-813a-56813dad66f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.3279645,  1.1193714,  1.3651992,  1.4757956,  1.2556211,\n",
              "       -0.576183 , -0.520399 ,  1.6278158], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "X_train[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oI1-HCaUYsiV",
      "metadata": {
        "id": "oI1-HCaUYsiV"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "Qkar158XyLwV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qkar158XyLwV",
        "outputId": "6e5fd7e3-28c3-4409-ce33-b8f5327434df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "print(tf.test.gpu_device_name())\n",
        "from numpy.random import seed\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "JGtCxoTQOkch",
      "metadata": {
        "id": "JGtCxoTQOkch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c4cbd2-b369-4b14-82a1-126e51dd62c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_8 (InputLayer)           [(None, 75, 8)]      0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization_84 (LayerN  (None, 75, 8)       16          ['input_8[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_84 (Multi  (None, 75, 8)       358         ['layer_normalization_84[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_84[0][0]'] \n",
            "                                                                                                  \n",
            " dense_266 (Dense)              (None, 75, 256)      2304        ['multi_head_attention_84[0][0]']\n",
            "                                                                                                  \n",
            " dense_267 (Dense)              (None, 75, 128)      32896       ['dense_266[0][0]']              \n",
            "                                                                                                  \n",
            " dense_268 (Dense)              (None, 75, 8)        1032        ['dense_267[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_168 (TFOp  (None, 75, 8)       0           ['multi_head_attention_84[0][0]',\n",
            " Lambda)                                                          'input_8[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_169 (TFOp  (None, 75, 8)       0           ['dense_268[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_168[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_85 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_169[0][0]'\n",
            " ormalization)                                                   ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_85 (Multi  (None, 75, 8)       358         ['layer_normalization_85[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_85[0][0]'] \n",
            "                                                                                                  \n",
            " dense_269 (Dense)              (None, 75, 256)      2304        ['multi_head_attention_85[0][0]']\n",
            "                                                                                                  \n",
            " dense_270 (Dense)              (None, 75, 128)      32896       ['dense_269[0][0]']              \n",
            "                                                                                                  \n",
            " dense_271 (Dense)              (None, 75, 8)        1032        ['dense_270[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_170 (TFOp  (None, 75, 8)       0           ['multi_head_attention_85[0][0]',\n",
            " Lambda)                                                          'tf.__operators__.add_169[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_171 (TFOp  (None, 75, 8)       0           ['dense_271[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_170[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_86 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_171[0][0]'\n",
            " ormalization)                                                   ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_86 (Multi  (None, 75, 8)       358         ['layer_normalization_86[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_86[0][0]'] \n",
            "                                                                                                  \n",
            " dense_272 (Dense)              (None, 75, 256)      2304        ['multi_head_attention_86[0][0]']\n",
            "                                                                                                  \n",
            " dense_273 (Dense)              (None, 75, 128)      32896       ['dense_272[0][0]']              \n",
            "                                                                                                  \n",
            " dense_274 (Dense)              (None, 75, 8)        1032        ['dense_273[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_172 (TFOp  (None, 75, 8)       0           ['multi_head_attention_86[0][0]',\n",
            " Lambda)                                                          'tf.__operators__.add_171[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_173 (TFOp  (None, 75, 8)       0           ['dense_274[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_172[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_87 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_173[0][0]'\n",
            " ormalization)                                                   ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_87 (Multi  (None, 75, 8)       358         ['layer_normalization_87[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_87[0][0]'] \n",
            "                                                                                                  \n",
            " dense_275 (Dense)              (None, 75, 256)      2304        ['multi_head_attention_87[0][0]']\n",
            "                                                                                                  \n",
            " dense_276 (Dense)              (None, 75, 128)      32896       ['dense_275[0][0]']              \n",
            "                                                                                                  \n",
            " dense_277 (Dense)              (None, 75, 8)        1032        ['dense_276[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_174 (TFOp  (None, 75, 8)       0           ['multi_head_attention_87[0][0]',\n",
            " Lambda)                                                          'tf.__operators__.add_173[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_175 (TFOp  (None, 75, 8)       0           ['dense_277[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_174[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_88 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_175[0][0]'\n",
            " ormalization)                                                   ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_88 (Multi  (None, 75, 8)       358         ['layer_normalization_88[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_88[0][0]'] \n",
            "                                                                                                  \n",
            " dense_278 (Dense)              (None, 75, 256)      2304        ['multi_head_attention_88[0][0]']\n",
            "                                                                                                  \n",
            " dense_279 (Dense)              (None, 75, 128)      32896       ['dense_278[0][0]']              \n",
            "                                                                                                  \n",
            " dense_280 (Dense)              (None, 75, 8)        1032        ['dense_279[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_176 (TFOp  (None, 75, 8)       0           ['multi_head_attention_88[0][0]',\n",
            " Lambda)                                                          'tf.__operators__.add_175[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_177 (TFOp  (None, 75, 8)       0           ['dense_280[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_176[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_89 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_177[0][0]'\n",
            " ormalization)                                                   ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_89 (Multi  (None, 75, 8)       358         ['layer_normalization_89[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_89[0][0]'] \n",
            "                                                                                                  \n",
            " dense_281 (Dense)              (None, 75, 256)      2304        ['multi_head_attention_89[0][0]']\n",
            "                                                                                                  \n",
            " dense_282 (Dense)              (None, 75, 128)      32896       ['dense_281[0][0]']              \n",
            "                                                                                                  \n",
            " dense_283 (Dense)              (None, 75, 8)        1032        ['dense_282[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_178 (TFOp  (None, 75, 8)       0           ['multi_head_attention_89[0][0]',\n",
            " Lambda)                                                          'tf.__operators__.add_177[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_179 (TFOp  (None, 75, 8)       0           ['dense_283[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_178[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_90 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_179[0][0]'\n",
            " ormalization)                                                   ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_90 (Multi  (None, 75, 8)       358         ['layer_normalization_90[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_90[0][0]'] \n",
            "                                                                                                  \n",
            " dense_284 (Dense)              (None, 75, 256)      2304        ['multi_head_attention_90[0][0]']\n",
            "                                                                                                  \n",
            " dense_285 (Dense)              (None, 75, 128)      32896       ['dense_284[0][0]']              \n",
            "                                                                                                  \n",
            " dense_286 (Dense)              (None, 75, 8)        1032        ['dense_285[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_180 (TFOp  (None, 75, 8)       0           ['multi_head_attention_90[0][0]',\n",
            " Lambda)                                                          'tf.__operators__.add_179[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_181 (TFOp  (None, 75, 8)       0           ['dense_286[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_180[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_91 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_181[0][0]'\n",
            " ormalization)                                                   ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_91 (Multi  (None, 75, 8)       358         ['layer_normalization_91[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_91[0][0]'] \n",
            "                                                                                                  \n",
            " dense_287 (Dense)              (None, 75, 256)      2304        ['multi_head_attention_91[0][0]']\n",
            "                                                                                                  \n",
            " dense_288 (Dense)              (None, 75, 128)      32896       ['dense_287[0][0]']              \n",
            "                                                                                                  \n",
            " dense_289 (Dense)              (None, 75, 8)        1032        ['dense_288[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_182 (TFOp  (None, 75, 8)       0           ['multi_head_attention_91[0][0]',\n",
            " Lambda)                                                          'tf.__operators__.add_181[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_183 (TFOp  (None, 75, 8)       0           ['dense_289[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_182[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_92 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_183[0][0]'\n",
            " ormalization)                                                   ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_92 (Multi  (None, 75, 8)       358         ['layer_normalization_92[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_92[0][0]'] \n",
            "                                                                                                  \n",
            " dense_290 (Dense)              (None, 75, 256)      2304        ['multi_head_attention_92[0][0]']\n",
            "                                                                                                  \n",
            " dense_291 (Dense)              (None, 75, 128)      32896       ['dense_290[0][0]']              \n",
            "                                                                                                  \n",
            " dense_292 (Dense)              (None, 75, 8)        1032        ['dense_291[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_184 (TFOp  (None, 75, 8)       0           ['multi_head_attention_92[0][0]',\n",
            " Lambda)                                                          'tf.__operators__.add_183[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_185 (TFOp  (None, 75, 8)       0           ['dense_292[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_184[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_93 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_185[0][0]'\n",
            " ormalization)                                                   ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_93 (Multi  (None, 75, 8)       358         ['layer_normalization_93[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_93[0][0]'] \n",
            "                                                                                                  \n",
            " dense_293 (Dense)              (None, 75, 256)      2304        ['multi_head_attention_93[0][0]']\n",
            "                                                                                                  \n",
            " dense_294 (Dense)              (None, 75, 128)      32896       ['dense_293[0][0]']              \n",
            "                                                                                                  \n",
            " dense_295 (Dense)              (None, 75, 8)        1032        ['dense_294[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_186 (TFOp  (None, 75, 8)       0           ['multi_head_attention_93[0][0]',\n",
            " Lambda)                                                          'tf.__operators__.add_185[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_187 (TFOp  (None, 75, 8)       0           ['dense_295[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_186[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_94 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_187[0][0]'\n",
            " ormalization)                                                   ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_94 (Multi  (None, 75, 8)       358         ['layer_normalization_94[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_94[0][0]'] \n",
            "                                                                                                  \n",
            " dense_296 (Dense)              (None, 75, 256)      2304        ['multi_head_attention_94[0][0]']\n",
            "                                                                                                  \n",
            " dense_297 (Dense)              (None, 75, 128)      32896       ['dense_296[0][0]']              \n",
            "                                                                                                  \n",
            " dense_298 (Dense)              (None, 75, 8)        1032        ['dense_297[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_188 (TFOp  (None, 75, 8)       0           ['multi_head_attention_94[0][0]',\n",
            " Lambda)                                                          'tf.__operators__.add_187[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_189 (TFOp  (None, 75, 8)       0           ['dense_298[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_188[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_95 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_189[0][0]'\n",
            " ormalization)                                                   ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_95 (Multi  (None, 75, 8)       358         ['layer_normalization_95[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_95[0][0]'] \n",
            "                                                                                                  \n",
            " dense_299 (Dense)              (None, 75, 256)      2304        ['multi_head_attention_95[0][0]']\n",
            "                                                                                                  \n",
            " dense_300 (Dense)              (None, 75, 128)      32896       ['dense_299[0][0]']              \n",
            "                                                                                                  \n",
            " dense_301 (Dense)              (None, 75, 8)        1032        ['dense_300[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_190 (TFOp  (None, 75, 8)       0           ['multi_head_attention_95[0][0]',\n",
            " Lambda)                                                          'tf.__operators__.add_189[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_191 (TFOp  (None, 75, 8)       0           ['dense_301[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_190[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_7 (Gl  (None, 75)          0           ['tf.__operators__.add_191[0][0]'\n",
            " obalAveragePooling1D)                                           ]                                \n",
            "                                                                                                  \n",
            " dense_302 (Dense)              (None, 128)          9728        ['global_average_pooling1d_7[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_303 (Dense)              (None, 3)            387         ['dense_302[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 449,387\n",
            "Trainable params: 449,387\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "3/3 [==============================] - 10s 892ms/step - loss: 23.8550 - accuracy: 0.7059 - val_loss: 23.2808 - val_accuracy: 0.6538\n",
            "Epoch 2/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 22.9977 - accuracy: 0.7059 - val_loss: 22.4264 - val_accuracy: 0.6538\n",
            "Epoch 3/500\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 22.2574 - accuracy: 0.7059 - val_loss: 21.6743 - val_accuracy: 0.6538\n",
            "Epoch 4/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 21.5173 - accuracy: 0.7059 - val_loss: 20.9739 - val_accuracy: 0.6538\n",
            "Epoch 5/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 20.7901 - accuracy: 0.7059 - val_loss: 20.3010 - val_accuracy: 0.6538\n",
            "Epoch 6/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 20.0872 - accuracy: 0.7059 - val_loss: 19.6372 - val_accuracy: 0.6538\n",
            "Epoch 7/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 19.4126 - accuracy: 0.7059 - val_loss: 18.9788 - val_accuracy: 0.6538\n",
            "Epoch 8/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 18.7581 - accuracy: 0.7059 - val_loss: 18.3469 - val_accuracy: 0.6538\n",
            "Epoch 9/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 18.1212 - accuracy: 0.7059 - val_loss: 17.7080 - val_accuracy: 0.6538\n",
            "Epoch 10/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 17.5033 - accuracy: 0.7059 - val_loss: 17.0977 - val_accuracy: 0.6538\n",
            "Epoch 11/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 16.9051 - accuracy: 0.7059 - val_loss: 16.5113 - val_accuracy: 0.6538\n",
            "Epoch 12/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 16.3289 - accuracy: 0.7059 - val_loss: 15.9524 - val_accuracy: 0.6538\n",
            "Epoch 13/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 15.7681 - accuracy: 0.7059 - val_loss: 15.3970 - val_accuracy: 0.6538\n",
            "Epoch 14/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 15.2298 - accuracy: 0.7059 - val_loss: 14.8725 - val_accuracy: 0.6538\n",
            "Epoch 15/500\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 14.7070 - accuracy: 0.7059 - val_loss: 14.3689 - val_accuracy: 0.6538\n",
            "Epoch 16/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 14.2026 - accuracy: 0.7059 - val_loss: 13.8891 - val_accuracy: 0.6538\n",
            "Epoch 17/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 13.7149 - accuracy: 0.7059 - val_loss: 13.4077 - val_accuracy: 0.6538\n",
            "Epoch 18/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 13.2429 - accuracy: 0.7059 - val_loss: 12.9548 - val_accuracy: 0.6538\n",
            "Epoch 19/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 12.7890 - accuracy: 0.7059 - val_loss: 12.5153 - val_accuracy: 0.6538\n",
            "Epoch 20/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 12.3535 - accuracy: 0.7059 - val_loss: 12.0934 - val_accuracy: 0.6538\n",
            "Epoch 21/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 11.9303 - accuracy: 0.7059 - val_loss: 11.6665 - val_accuracy: 0.6538\n",
            "Epoch 22/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 11.5179 - accuracy: 0.7059 - val_loss: 11.2621 - val_accuracy: 0.6538\n",
            "Epoch 23/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 11.1217 - accuracy: 0.7059 - val_loss: 10.8822 - val_accuracy: 0.6538\n",
            "Epoch 24/500\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 10.7408 - accuracy: 0.7059 - val_loss: 10.5160 - val_accuracy: 0.6538\n",
            "Epoch 25/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 10.3732 - accuracy: 0.7059 - val_loss: 10.1572 - val_accuracy: 0.6538\n",
            "Epoch 26/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 10.0180 - accuracy: 0.7059 - val_loss: 9.8072 - val_accuracy: 0.6538\n",
            "Epoch 27/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 9.6775 - accuracy: 0.7059 - val_loss: 9.4776 - val_accuracy: 0.6538\n",
            "Epoch 28/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 9.3438 - accuracy: 0.7059 - val_loss: 9.1642 - val_accuracy: 0.6538\n",
            "Epoch 29/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 9.0290 - accuracy: 0.7059 - val_loss: 8.8561 - val_accuracy: 0.6538\n",
            "Epoch 30/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 8.7214 - accuracy: 0.7059 - val_loss: 8.5388 - val_accuracy: 0.6538\n",
            "Epoch 31/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 8.4217 - accuracy: 0.7059 - val_loss: 8.2485 - val_accuracy: 0.6538\n",
            "Epoch 32/500\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 8.1401 - accuracy: 0.7059 - val_loss: 7.9815 - val_accuracy: 0.6538\n",
            "Epoch 33/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 7.8622 - accuracy: 0.7059 - val_loss: 7.7078 - val_accuracy: 0.6538\n",
            "Epoch 34/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 7.5964 - accuracy: 0.7059 - val_loss: 7.4466 - val_accuracy: 0.6538\n",
            "Epoch 35/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 7.3360 - accuracy: 0.7059 - val_loss: 7.2100 - val_accuracy: 0.6538\n",
            "Epoch 36/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 7.0926 - accuracy: 0.7059 - val_loss: 6.9759 - val_accuracy: 0.6538\n",
            "Epoch 37/500\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 6.8540 - accuracy: 0.7059 - val_loss: 6.7270 - val_accuracy: 0.6538\n",
            "Epoch 38/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 6.6277 - accuracy: 0.7059 - val_loss: 6.5000 - val_accuracy: 0.6538\n",
            "Epoch 39/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 6.4042 - accuracy: 0.7059 - val_loss: 6.2825 - val_accuracy: 0.6538\n",
            "Epoch 40/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 6.1862 - accuracy: 0.7059 - val_loss: 6.0913 - val_accuracy: 0.6538\n",
            "Epoch 41/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 5.9891 - accuracy: 0.7059 - val_loss: 5.8967 - val_accuracy: 0.6538\n",
            "Epoch 42/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 5.7863 - accuracy: 0.7059 - val_loss: 5.7069 - val_accuracy: 0.6538\n",
            "Epoch 43/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 5.5977 - accuracy: 0.7059 - val_loss: 5.5178 - val_accuracy: 0.6538\n",
            "Epoch 44/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 5.4171 - accuracy: 0.7059 - val_loss: 5.3238 - val_accuracy: 0.6538\n",
            "Epoch 45/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 5.2340 - accuracy: 0.7059 - val_loss: 5.1499 - val_accuracy: 0.6538\n",
            "Epoch 46/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 5.0609 - accuracy: 0.7059 - val_loss: 4.9882 - val_accuracy: 0.6538\n",
            "Epoch 47/500\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 4.8992 - accuracy: 0.7059 - val_loss: 4.8371 - val_accuracy: 0.6538\n",
            "Epoch 48/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 4.7440 - accuracy: 0.7059 - val_loss: 4.6966 - val_accuracy: 0.6538\n",
            "Epoch 49/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 4.5886 - accuracy: 0.7059 - val_loss: 4.5449 - val_accuracy: 0.6538\n",
            "Epoch 50/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 4.4431 - accuracy: 0.7059 - val_loss: 4.4076 - val_accuracy: 0.6538\n",
            "Epoch 51/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 4.3006 - accuracy: 0.7059 - val_loss: 4.2629 - val_accuracy: 0.6538\n",
            "Epoch 52/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 4.1662 - accuracy: 0.7059 - val_loss: 4.1190 - val_accuracy: 0.6538\n",
            "Epoch 53/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 4.0374 - accuracy: 0.7059 - val_loss: 3.9911 - val_accuracy: 0.6538\n",
            "Epoch 54/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 3.9107 - accuracy: 0.7059 - val_loss: 3.8727 - val_accuracy: 0.6538\n",
            "Epoch 55/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 3.7918 - accuracy: 0.7059 - val_loss: 3.7624 - val_accuracy: 0.6538\n",
            "Epoch 56/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 3.6743 - accuracy: 0.7059 - val_loss: 3.6706 - val_accuracy: 0.6538\n",
            "Epoch 57/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 3.5662 - accuracy: 0.7059 - val_loss: 3.5547 - val_accuracy: 0.6538\n",
            "Epoch 58/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 3.4552 - accuracy: 0.7059 - val_loss: 3.4401 - val_accuracy: 0.6538\n",
            "Epoch 59/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 3.3578 - accuracy: 0.7059 - val_loss: 3.3414 - val_accuracy: 0.6538\n",
            "Epoch 60/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 3.2560 - accuracy: 0.7059 - val_loss: 3.2507 - val_accuracy: 0.6538\n",
            "Epoch 61/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 3.1598 - accuracy: 0.7059 - val_loss: 3.1643 - val_accuracy: 0.6538\n",
            "Epoch 62/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 3.0663 - accuracy: 0.7059 - val_loss: 3.0732 - val_accuracy: 0.6538\n",
            "Epoch 63/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 2.9812 - accuracy: 0.7059 - val_loss: 2.9822 - val_accuracy: 0.6538\n",
            "Epoch 64/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 2.8958 - accuracy: 0.7059 - val_loss: 2.9014 - val_accuracy: 0.6538\n",
            "Epoch 65/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 2.8138 - accuracy: 0.7059 - val_loss: 2.8340 - val_accuracy: 0.6538\n",
            "Epoch 66/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 2.7412 - accuracy: 0.7059 - val_loss: 2.7579 - val_accuracy: 0.6538\n",
            "Epoch 67/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 2.6638 - accuracy: 0.7059 - val_loss: 2.6823 - val_accuracy: 0.6538\n",
            "Epoch 68/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 2.5986 - accuracy: 0.7059 - val_loss: 2.6109 - val_accuracy: 0.6538\n",
            "Epoch 69/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 2.5186 - accuracy: 0.7059 - val_loss: 2.5614 - val_accuracy: 0.6538\n",
            "Epoch 70/500\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 2.4654 - accuracy: 0.7059 - val_loss: 2.5051 - val_accuracy: 0.6538\n",
            "Epoch 71/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 2.3949 - accuracy: 0.7059 - val_loss: 2.4128 - val_accuracy: 0.6538\n",
            "Epoch 72/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 2.3339 - accuracy: 0.7059 - val_loss: 2.3503 - val_accuracy: 0.6538\n",
            "Epoch 73/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 2.2774 - accuracy: 0.7059 - val_loss: 2.3062 - val_accuracy: 0.6538\n",
            "Epoch 74/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 2.2164 - accuracy: 0.7059 - val_loss: 2.2648 - val_accuracy: 0.6538\n",
            "Epoch 75/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 2.1738 - accuracy: 0.7059 - val_loss: 2.1977 - val_accuracy: 0.6538\n",
            "Epoch 76/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 2.1178 - accuracy: 0.7059 - val_loss: 2.1472 - val_accuracy: 0.6538\n",
            "Epoch 77/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 2.0693 - accuracy: 0.7059 - val_loss: 2.1008 - val_accuracy: 0.6538\n",
            "Epoch 78/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 2.0173 - accuracy: 0.7059 - val_loss: 2.0698 - val_accuracy: 0.6538\n",
            "Epoch 79/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 1.9739 - accuracy: 0.7059 - val_loss: 2.0104 - val_accuracy: 0.6538\n",
            "Epoch 80/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 1.9254 - accuracy: 0.7059 - val_loss: 1.9637 - val_accuracy: 0.6538\n",
            "Epoch 81/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 1.8820 - accuracy: 0.7059 - val_loss: 1.9294 - val_accuracy: 0.6538\n",
            "Epoch 82/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 1.8497 - accuracy: 0.7059 - val_loss: 1.9010 - val_accuracy: 0.6538\n",
            "Epoch 83/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 1.8082 - accuracy: 0.7059 - val_loss: 1.8716 - val_accuracy: 0.6538\n",
            "Epoch 84/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 1.7670 - accuracy: 0.7059 - val_loss: 1.8327 - val_accuracy: 0.6538\n",
            "Epoch 85/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 1.7323 - accuracy: 0.7059 - val_loss: 1.7927 - val_accuracy: 0.6538\n",
            "Epoch 86/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 1.6971 - accuracy: 0.7059 - val_loss: 1.7662 - val_accuracy: 0.6538\n",
            "Epoch 87/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 1.6674 - accuracy: 0.7059 - val_loss: 1.7391 - val_accuracy: 0.6538\n",
            "Epoch 88/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 1.6351 - accuracy: 0.7059 - val_loss: 1.7190 - val_accuracy: 0.6538\n",
            "Epoch 89/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 1.6062 - accuracy: 0.7059 - val_loss: 1.6902 - val_accuracy: 0.6538\n",
            "Epoch 90/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 1.5741 - accuracy: 0.7059 - val_loss: 1.6467 - val_accuracy: 0.6538\n",
            "Epoch 91/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 1.5496 - accuracy: 0.7059 - val_loss: 1.6146 - val_accuracy: 0.6538\n",
            "Epoch 92/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 1.5233 - accuracy: 0.7059 - val_loss: 1.5927 - val_accuracy: 0.6538\n",
            "Epoch 93/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 1.4959 - accuracy: 0.7059 - val_loss: 1.5752 - val_accuracy: 0.6538\n",
            "Epoch 94/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 1.4767 - accuracy: 0.7059 - val_loss: 1.5657 - val_accuracy: 0.6538\n",
            "Epoch 95/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 1.4489 - accuracy: 0.7059 - val_loss: 1.5673 - val_accuracy: 0.6538\n",
            "Epoch 96/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 1.4309 - accuracy: 0.7059 - val_loss: 1.5247 - val_accuracy: 0.6538\n",
            "Epoch 97/500\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 1.4058 - accuracy: 0.7059 - val_loss: 1.4943 - val_accuracy: 0.6538\n",
            "Epoch 98/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 1.3894 - accuracy: 0.7059 - val_loss: 1.4830 - val_accuracy: 0.6538\n",
            "Epoch 99/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 1.3630 - accuracy: 0.7059 - val_loss: 1.4719 - val_accuracy: 0.6538\n",
            "Epoch 100/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 1.3471 - accuracy: 0.7059 - val_loss: 1.4669 - val_accuracy: 0.6538\n",
            "Epoch 101/500\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 1.3287 - accuracy: 0.7059 - val_loss: 1.4495 - val_accuracy: 0.6538\n",
            "Epoch 102/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 1.3089 - accuracy: 0.7059 - val_loss: 1.4291 - val_accuracy: 0.6538\n",
            "Epoch 103/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 1.2941 - accuracy: 0.7059 - val_loss: 1.4136 - val_accuracy: 0.6538\n",
            "Epoch 104/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 1.2758 - accuracy: 0.7059 - val_loss: 1.3947 - val_accuracy: 0.6538\n",
            "Epoch 105/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 1.2622 - accuracy: 0.7059 - val_loss: 1.3833 - val_accuracy: 0.6538\n",
            "Epoch 106/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 1.2451 - accuracy: 0.7059 - val_loss: 1.3742 - val_accuracy: 0.6538\n",
            "Epoch 107/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 1.2332 - accuracy: 0.7059 - val_loss: 1.3592 - val_accuracy: 0.6538\n",
            "Epoch 108/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 1.2182 - accuracy: 0.7059 - val_loss: 1.3592 - val_accuracy: 0.6538\n",
            "Epoch 109/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 1.2093 - accuracy: 0.7059 - val_loss: 1.3538 - val_accuracy: 0.6538\n",
            "Epoch 110/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 1.1957 - accuracy: 0.7059 - val_loss: 1.3304 - val_accuracy: 0.6538\n",
            "Epoch 111/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 1.1863 - accuracy: 0.7059 - val_loss: 1.3212 - val_accuracy: 0.6538\n",
            "Epoch 112/500\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 1.1735 - accuracy: 0.7059 - val_loss: 1.3213 - val_accuracy: 0.6538\n",
            "Epoch 113/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 1.1604 - accuracy: 0.7059 - val_loss: 1.3080 - val_accuracy: 0.6538\n",
            "Epoch 114/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 1.1469 - accuracy: 0.7059 - val_loss: 1.2782 - val_accuracy: 0.6538\n",
            "Epoch 115/500\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 1.1396 - accuracy: 0.7118 - val_loss: 1.2642 - val_accuracy: 0.6538\n",
            "Epoch 116/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 1.1274 - accuracy: 0.7118 - val_loss: 1.2788 - val_accuracy: 0.6538\n",
            "Epoch 117/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 1.1179 - accuracy: 0.7059 - val_loss: 1.2842 - val_accuracy: 0.6538\n",
            "Epoch 118/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 1.1082 - accuracy: 0.7059 - val_loss: 1.2620 - val_accuracy: 0.6538\n",
            "Epoch 119/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 1.0994 - accuracy: 0.7059 - val_loss: 1.2558 - val_accuracy: 0.6538\n",
            "Epoch 120/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 1.0871 - accuracy: 0.7059 - val_loss: 1.2666 - val_accuracy: 0.6538\n",
            "Epoch 121/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 1.0827 - accuracy: 0.7059 - val_loss: 1.2563 - val_accuracy: 0.6538\n",
            "Epoch 122/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 1.0709 - accuracy: 0.7059 - val_loss: 1.2299 - val_accuracy: 0.6538\n",
            "Epoch 123/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 1.0657 - accuracy: 0.7118 - val_loss: 1.2215 - val_accuracy: 0.6538\n",
            "Epoch 124/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 1.0681 - accuracy: 0.7118 - val_loss: 1.2337 - val_accuracy: 0.6538\n",
            "Epoch 125/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 1.0479 - accuracy: 0.7059 - val_loss: 1.2239 - val_accuracy: 0.6538\n",
            "Epoch 126/500\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 1.0459 - accuracy: 0.7176 - val_loss: 1.2212 - val_accuracy: 0.6538\n",
            "Epoch 127/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 1.0355 - accuracy: 0.7118 - val_loss: 1.2392 - val_accuracy: 0.6538\n",
            "Epoch 128/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 1.0369 - accuracy: 0.7059 - val_loss: 1.2251 - val_accuracy: 0.6538\n",
            "Epoch 129/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 1.0249 - accuracy: 0.7118 - val_loss: 1.1885 - val_accuracy: 0.6538\n",
            "Epoch 130/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 1.0176 - accuracy: 0.7176 - val_loss: 1.1953 - val_accuracy: 0.6538\n",
            "Epoch 131/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 1.0091 - accuracy: 0.7118 - val_loss: 1.2072 - val_accuracy: 0.6538\n",
            "Epoch 132/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 1.0077 - accuracy: 0.7118 - val_loss: 1.1829 - val_accuracy: 0.6538\n",
            "Epoch 133/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.9981 - accuracy: 0.7118 - val_loss: 1.2030 - val_accuracy: 0.6538\n",
            "Epoch 134/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.9899 - accuracy: 0.7118 - val_loss: 1.2032 - val_accuracy: 0.6538\n",
            "Epoch 135/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.9934 - accuracy: 0.7118 - val_loss: 1.2173 - val_accuracy: 0.6538\n",
            "Epoch 136/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.9916 - accuracy: 0.7118 - val_loss: 1.2407 - val_accuracy: 0.6538\n",
            "Epoch 137/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.9895 - accuracy: 0.7235 - val_loss: 1.1593 - val_accuracy: 0.6538\n",
            "Epoch 138/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.9852 - accuracy: 0.7118 - val_loss: 1.1421 - val_accuracy: 0.6538\n",
            "Epoch 139/500\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.9724 - accuracy: 0.7176 - val_loss: 1.1762 - val_accuracy: 0.6538\n",
            "Epoch 140/500\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.9670 - accuracy: 0.7235 - val_loss: 1.1701 - val_accuracy: 0.6538\n",
            "Epoch 141/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.9576 - accuracy: 0.7235 - val_loss: 1.1558 - val_accuracy: 0.6538\n",
            "Epoch 142/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.9515 - accuracy: 0.7294 - val_loss: 1.1566 - val_accuracy: 0.6538\n",
            "Epoch 143/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.9506 - accuracy: 0.7294 - val_loss: 1.1775 - val_accuracy: 0.6538\n",
            "Epoch 144/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.9414 - accuracy: 0.7294 - val_loss: 1.2075 - val_accuracy: 0.6538\n",
            "Epoch 145/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.9401 - accuracy: 0.7235 - val_loss: 1.1771 - val_accuracy: 0.6538\n",
            "Epoch 146/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.9405 - accuracy: 0.7235 - val_loss: 1.1725 - val_accuracy: 0.6538\n",
            "Epoch 147/500\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.9268 - accuracy: 0.7235 - val_loss: 1.1823 - val_accuracy: 0.6538\n",
            "Epoch 148/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.9285 - accuracy: 0.7235 - val_loss: 1.1501 - val_accuracy: 0.6538\n",
            "Epoch 149/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.9226 - accuracy: 0.7294 - val_loss: 1.1723 - val_accuracy: 0.6538\n",
            "Epoch 150/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.9124 - accuracy: 0.7294 - val_loss: 1.1559 - val_accuracy: 0.6346\n",
            "Epoch 151/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.9122 - accuracy: 0.7294 - val_loss: 1.1765 - val_accuracy: 0.6538\n",
            "Epoch 152/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.9052 - accuracy: 0.7353 - val_loss: 1.2158 - val_accuracy: 0.6538\n",
            "Epoch 153/500\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.9041 - accuracy: 0.7294 - val_loss: 1.2008 - val_accuracy: 0.6538\n",
            "Epoch 154/500\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.8988 - accuracy: 0.7294 - val_loss: 1.1612 - val_accuracy: 0.6538\n",
            "Epoch 155/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.8926 - accuracy: 0.7294 - val_loss: 1.1840 - val_accuracy: 0.6346\n",
            "Epoch 156/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.8917 - accuracy: 0.7353 - val_loss: 1.1801 - val_accuracy: 0.6346\n",
            "Epoch 157/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.8879 - accuracy: 0.7412 - val_loss: 1.1746 - val_accuracy: 0.6346\n",
            "Epoch 158/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.8796 - accuracy: 0.7353 - val_loss: 1.1904 - val_accuracy: 0.6538\n",
            "Epoch 159/500\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.8752 - accuracy: 0.7353 - val_loss: 1.2122 - val_accuracy: 0.6346\n",
            "Epoch 160/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.8741 - accuracy: 0.7412 - val_loss: 1.1821 - val_accuracy: 0.6346\n",
            "Epoch 161/500\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.8719 - accuracy: 0.7412 - val_loss: 1.1724 - val_accuracy: 0.6346\n",
            "Epoch 162/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.8633 - accuracy: 0.7471 - val_loss: 1.2063 - val_accuracy: 0.6538\n",
            "Epoch 163/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.8660 - accuracy: 0.7471 - val_loss: 1.1582 - val_accuracy: 0.6346\n",
            "Epoch 164/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.8551 - accuracy: 0.7471 - val_loss: 1.2001 - val_accuracy: 0.6346\n",
            "Epoch 165/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.8532 - accuracy: 0.7471 - val_loss: 1.1791 - val_accuracy: 0.6346\n",
            "Epoch 166/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.8508 - accuracy: 0.7412 - val_loss: 1.1969 - val_accuracy: 0.6154\n",
            "Epoch 167/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.8568 - accuracy: 0.7412 - val_loss: 1.2116 - val_accuracy: 0.6154\n",
            "Epoch 168/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.8466 - accuracy: 0.7529 - val_loss: 1.1530 - val_accuracy: 0.5769\n",
            "Epoch 169/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.8445 - accuracy: 0.7529 - val_loss: 1.2057 - val_accuracy: 0.6154\n",
            "Epoch 170/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.8323 - accuracy: 0.7471 - val_loss: 1.2264 - val_accuracy: 0.6346\n",
            "Epoch 171/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.8261 - accuracy: 0.7588 - val_loss: 1.1671 - val_accuracy: 0.5769\n",
            "Epoch 172/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.8260 - accuracy: 0.7588 - val_loss: 1.1747 - val_accuracy: 0.5769\n",
            "Epoch 173/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.8190 - accuracy: 0.7588 - val_loss: 1.2226 - val_accuracy: 0.5385\n",
            "Epoch 174/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.8169 - accuracy: 0.7529 - val_loss: 1.2130 - val_accuracy: 0.5769\n",
            "Epoch 175/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.8096 - accuracy: 0.7647 - val_loss: 1.2211 - val_accuracy: 0.5769\n",
            "Epoch 176/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.8047 - accuracy: 0.7706 - val_loss: 1.2243 - val_accuracy: 0.5769\n",
            "Epoch 177/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.7955 - accuracy: 0.7647 - val_loss: 1.2397 - val_accuracy: 0.5769\n",
            "Epoch 178/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.7903 - accuracy: 0.7647 - val_loss: 1.2608 - val_accuracy: 0.5769\n",
            "Epoch 179/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.7900 - accuracy: 0.7765 - val_loss: 1.2548 - val_accuracy: 0.5577\n",
            "Epoch 180/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.7822 - accuracy: 0.7706 - val_loss: 1.2673 - val_accuracy: 0.5577\n",
            "Epoch 181/500\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.7867 - accuracy: 0.7588 - val_loss: 1.2733 - val_accuracy: 0.5577\n",
            "Epoch 182/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.7836 - accuracy: 0.7765 - val_loss: 1.2201 - val_accuracy: 0.5769\n",
            "Epoch 183/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.7700 - accuracy: 0.7765 - val_loss: 1.2964 - val_accuracy: 0.5769\n",
            "Epoch 184/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.7745 - accuracy: 0.7706 - val_loss: 1.2948 - val_accuracy: 0.5577\n",
            "Epoch 185/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.7579 - accuracy: 0.7824 - val_loss: 1.2901 - val_accuracy: 0.5385\n",
            "Epoch 186/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.7602 - accuracy: 0.7941 - val_loss: 1.3115 - val_accuracy: 0.5577\n",
            "Epoch 187/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.7473 - accuracy: 0.7941 - val_loss: 1.3102 - val_accuracy: 0.5577\n",
            "Epoch 188/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.7507 - accuracy: 0.7824 - val_loss: 1.2902 - val_accuracy: 0.5385\n",
            "Epoch 189/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.7539 - accuracy: 0.7941 - val_loss: 1.3282 - val_accuracy: 0.5577\n",
            "Epoch 190/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.7367 - accuracy: 0.7941 - val_loss: 1.3125 - val_accuracy: 0.5192\n",
            "Epoch 191/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.7317 - accuracy: 0.7824 - val_loss: 1.3041 - val_accuracy: 0.5192\n",
            "Epoch 192/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.7285 - accuracy: 0.7882 - val_loss: 1.3539 - val_accuracy: 0.5577\n",
            "Epoch 193/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.7246 - accuracy: 0.7941 - val_loss: 1.3576 - val_accuracy: 0.5385\n",
            "Epoch 194/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.7211 - accuracy: 0.7882 - val_loss: 1.3463 - val_accuracy: 0.5192\n",
            "Epoch 195/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.7056 - accuracy: 0.7882 - val_loss: 1.3766 - val_accuracy: 0.5385\n",
            "Epoch 196/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.7307 - accuracy: 0.7765 - val_loss: 1.2996 - val_accuracy: 0.5192\n",
            "Epoch 197/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.7263 - accuracy: 0.7824 - val_loss: 1.2667 - val_accuracy: 0.5577\n",
            "Epoch 198/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.7176 - accuracy: 0.7706 - val_loss: 1.3765 - val_accuracy: 0.5385\n",
            "Epoch 199/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.6995 - accuracy: 0.7824 - val_loss: 1.3743 - val_accuracy: 0.5000\n",
            "Epoch 200/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.7047 - accuracy: 0.7706 - val_loss: 1.3747 - val_accuracy: 0.5192\n",
            "Epoch 201/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.6971 - accuracy: 0.8059 - val_loss: 1.4018 - val_accuracy: 0.5192\n",
            "Epoch 202/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.6897 - accuracy: 0.8000 - val_loss: 1.4574 - val_accuracy: 0.5385\n",
            "Epoch 203/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.6763 - accuracy: 0.8000 - val_loss: 1.3717 - val_accuracy: 0.5385\n",
            "Epoch 204/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.6788 - accuracy: 0.8059 - val_loss: 1.4132 - val_accuracy: 0.5192\n",
            "Epoch 205/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.6675 - accuracy: 0.7882 - val_loss: 1.4460 - val_accuracy: 0.5385\n",
            "Epoch 206/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.6613 - accuracy: 0.7941 - val_loss: 1.4574 - val_accuracy: 0.5192\n",
            "Epoch 207/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.6560 - accuracy: 0.7882 - val_loss: 1.4977 - val_accuracy: 0.5385\n",
            "Epoch 208/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.6438 - accuracy: 0.8000 - val_loss: 1.5365 - val_accuracy: 0.5192\n",
            "Epoch 209/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.6406 - accuracy: 0.8118 - val_loss: 1.5539 - val_accuracy: 0.5192\n",
            "Epoch 210/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.6372 - accuracy: 0.8059 - val_loss: 1.5508 - val_accuracy: 0.5192\n",
            "Epoch 211/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.6287 - accuracy: 0.8118 - val_loss: 1.5832 - val_accuracy: 0.5192\n",
            "Epoch 212/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.6229 - accuracy: 0.8059 - val_loss: 1.6087 - val_accuracy: 0.5192\n",
            "Epoch 213/500\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.6198 - accuracy: 0.8118 - val_loss: 1.6327 - val_accuracy: 0.5192\n",
            "Epoch 214/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.6078 - accuracy: 0.8176 - val_loss: 1.6725 - val_accuracy: 0.5192\n",
            "Epoch 215/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.6005 - accuracy: 0.8118 - val_loss: 1.7099 - val_accuracy: 0.5385\n",
            "Epoch 216/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.5965 - accuracy: 0.8118 - val_loss: 1.7322 - val_accuracy: 0.5192\n",
            "Epoch 217/500\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.5851 - accuracy: 0.8118 - val_loss: 1.7755 - val_accuracy: 0.5192\n",
            "Epoch 218/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.5777 - accuracy: 0.8176 - val_loss: 1.8108 - val_accuracy: 0.5192\n",
            "Epoch 219/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.5704 - accuracy: 0.8118 - val_loss: 1.8912 - val_accuracy: 0.5192\n",
            "Epoch 220/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.5658 - accuracy: 0.8118 - val_loss: 1.9144 - val_accuracy: 0.5192\n",
            "Epoch 221/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.5566 - accuracy: 0.8235 - val_loss: 1.9729 - val_accuracy: 0.5192\n",
            "Epoch 222/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.5451 - accuracy: 0.8235 - val_loss: 2.0368 - val_accuracy: 0.5192\n",
            "Epoch 223/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.5398 - accuracy: 0.8294 - val_loss: 2.0383 - val_accuracy: 0.5192\n",
            "Epoch 224/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.5387 - accuracy: 0.8412 - val_loss: 2.0784 - val_accuracy: 0.5000\n",
            "Epoch 225/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.5299 - accuracy: 0.8529 - val_loss: 2.1221 - val_accuracy: 0.5000\n",
            "Epoch 226/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.5170 - accuracy: 0.8471 - val_loss: 2.1880 - val_accuracy: 0.5000\n",
            "Epoch 227/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.5125 - accuracy: 0.8471 - val_loss: 2.2135 - val_accuracy: 0.4615\n",
            "Epoch 228/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.5020 - accuracy: 0.8765 - val_loss: 2.2655 - val_accuracy: 0.4808\n",
            "Epoch 229/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4938 - accuracy: 0.8706 - val_loss: 2.3170 - val_accuracy: 0.4615\n",
            "Epoch 230/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4874 - accuracy: 0.8882 - val_loss: 2.3486 - val_accuracy: 0.4808\n",
            "Epoch 231/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4807 - accuracy: 0.8882 - val_loss: 2.4035 - val_accuracy: 0.4808\n",
            "Epoch 232/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4773 - accuracy: 0.9000 - val_loss: 2.4271 - val_accuracy: 0.4615\n",
            "Epoch 233/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4643 - accuracy: 0.9000 - val_loss: 2.4792 - val_accuracy: 0.4808\n",
            "Epoch 234/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.4602 - accuracy: 0.8941 - val_loss: 2.5630 - val_accuracy: 0.4615\n",
            "Epoch 235/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.4541 - accuracy: 0.9118 - val_loss: 2.6287 - val_accuracy: 0.4615\n",
            "Epoch 236/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.4415 - accuracy: 0.9059 - val_loss: 2.7045 - val_accuracy: 0.4615\n",
            "Epoch 237/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.4338 - accuracy: 0.9059 - val_loss: 2.7686 - val_accuracy: 0.4615\n",
            "Epoch 238/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4241 - accuracy: 0.9059 - val_loss: 2.8217 - val_accuracy: 0.4615\n",
            "Epoch 239/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.4182 - accuracy: 0.9176 - val_loss: 2.8559 - val_accuracy: 0.4615\n",
            "Epoch 240/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 0.4097 - accuracy: 0.9176 - val_loss: 2.9134 - val_accuracy: 0.4615\n",
            "Epoch 241/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.4098 - accuracy: 0.9176 - val_loss: 2.9715 - val_accuracy: 0.4615\n",
            "Epoch 242/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3934 - accuracy: 0.9176 - val_loss: 3.0597 - val_accuracy: 0.4615\n",
            "Epoch 243/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3903 - accuracy: 0.9176 - val_loss: 3.1163 - val_accuracy: 0.4615\n",
            "Epoch 244/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.3876 - accuracy: 0.9176 - val_loss: 3.1682 - val_accuracy: 0.4423\n",
            "Epoch 245/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3819 - accuracy: 0.9118 - val_loss: 3.2128 - val_accuracy: 0.4615\n",
            "Epoch 246/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3843 - accuracy: 0.9000 - val_loss: 3.2639 - val_accuracy: 0.4808\n",
            "Epoch 247/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3800 - accuracy: 0.9176 - val_loss: 3.3342 - val_accuracy: 0.4808\n",
            "Epoch 248/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3574 - accuracy: 0.9176 - val_loss: 3.4397 - val_accuracy: 0.4423\n",
            "Epoch 249/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3574 - accuracy: 0.9176 - val_loss: 3.5169 - val_accuracy: 0.5000\n",
            "Epoch 250/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3684 - accuracy: 0.9059 - val_loss: 3.5269 - val_accuracy: 0.5000\n",
            "Epoch 251/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3443 - accuracy: 0.9176 - val_loss: 3.5203 - val_accuracy: 0.4808\n",
            "Epoch 252/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.3572 - accuracy: 0.9176 - val_loss: 3.6075 - val_accuracy: 0.5000\n",
            "Epoch 253/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3280 - accuracy: 0.9412 - val_loss: 3.6476 - val_accuracy: 0.5000\n",
            "Epoch 254/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.3263 - accuracy: 0.9412 - val_loss: 3.7356 - val_accuracy: 0.4808\n",
            "Epoch 255/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.3214 - accuracy: 0.9471 - val_loss: 3.8482 - val_accuracy: 0.5000\n",
            "Epoch 256/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.3098 - accuracy: 0.9471 - val_loss: 3.9134 - val_accuracy: 0.4615\n",
            "Epoch 257/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3066 - accuracy: 0.9529 - val_loss: 3.9859 - val_accuracy: 0.4615\n",
            "Epoch 258/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.3041 - accuracy: 0.9471 - val_loss: 4.0815 - val_accuracy: 0.5000\n",
            "Epoch 259/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.2958 - accuracy: 0.9529 - val_loss: 4.1469 - val_accuracy: 0.4808\n",
            "Epoch 260/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3023 - accuracy: 0.9588 - val_loss: 4.1753 - val_accuracy: 0.4615\n",
            "Epoch 261/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.2980 - accuracy: 0.9529 - val_loss: 4.2143 - val_accuracy: 0.4808\n",
            "Epoch 262/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.2904 - accuracy: 0.9588 - val_loss: 4.2784 - val_accuracy: 0.4423\n",
            "Epoch 263/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.2842 - accuracy: 0.9588 - val_loss: 4.3974 - val_accuracy: 0.4615\n",
            "Epoch 264/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.2789 - accuracy: 0.9588 - val_loss: 4.4627 - val_accuracy: 0.4615\n",
            "Epoch 265/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.2735 - accuracy: 0.9588 - val_loss: 4.4830 - val_accuracy: 0.4615\n",
            "Epoch 266/500\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.2734 - accuracy: 0.9588 - val_loss: 4.5632 - val_accuracy: 0.4615\n",
            "Epoch 267/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.2718 - accuracy: 0.9588 - val_loss: 4.6552 - val_accuracy: 0.4423\n",
            "Epoch 268/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.2658 - accuracy: 0.9588 - val_loss: 4.7320 - val_accuracy: 0.4423\n",
            "Epoch 269/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.2680 - accuracy: 0.9588 - val_loss: 4.8011 - val_accuracy: 0.4808\n",
            "Epoch 270/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.2600 - accuracy: 0.9588 - val_loss: 4.8699 - val_accuracy: 0.4615\n",
            "Epoch 271/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.2590 - accuracy: 0.9588 - val_loss: 4.9294 - val_accuracy: 0.4423\n",
            "Epoch 272/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.2594 - accuracy: 0.9588 - val_loss: 4.9772 - val_accuracy: 0.4615\n",
            "Epoch 273/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.2604 - accuracy: 0.9588 - val_loss: 5.0301 - val_accuracy: 0.4423\n",
            "Epoch 274/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.2509 - accuracy: 0.9588 - val_loss: 5.0866 - val_accuracy: 0.4423\n",
            "Epoch 275/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.2537 - accuracy: 0.9588 - val_loss: 5.1328 - val_accuracy: 0.4423\n",
            "Epoch 276/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.2651 - accuracy: 0.9588 - val_loss: 5.2013 - val_accuracy: 0.4615\n",
            "Epoch 277/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.2471 - accuracy: 0.9588 - val_loss: 5.2063 - val_accuracy: 0.4423\n",
            "Epoch 278/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.2580 - accuracy: 0.9647 - val_loss: 5.2449 - val_accuracy: 0.4423\n",
            "Epoch 279/500\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.2475 - accuracy: 0.9588 - val_loss: 5.3680 - val_accuracy: 0.5000\n",
            "Epoch 280/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.2591 - accuracy: 0.9588 - val_loss: 5.3642 - val_accuracy: 0.5000\n",
            "Epoch 281/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.2432 - accuracy: 0.9588 - val_loss: 5.3160 - val_accuracy: 0.4423\n",
            "Epoch 282/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.2534 - accuracy: 0.9529 - val_loss: 5.3782 - val_accuracy: 0.4615\n",
            "Epoch 283/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.2579 - accuracy: 0.9588 - val_loss: 5.4956 - val_accuracy: 0.5000\n",
            "Epoch 284/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.2479 - accuracy: 0.9588 - val_loss: 5.3859 - val_accuracy: 0.4423\n",
            "Epoch 285/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.2418 - accuracy: 0.9647 - val_loss: 5.3678 - val_accuracy: 0.4423\n",
            "Epoch 286/500\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.2400 - accuracy: 0.9647 - val_loss: 5.4884 - val_accuracy: 0.5000\n",
            "Epoch 287/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.2356 - accuracy: 0.9588 - val_loss: 5.5393 - val_accuracy: 0.5000\n",
            "Epoch 288/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.2430 - accuracy: 0.9588 - val_loss: 5.5048 - val_accuracy: 0.4615\n",
            "Epoch 289/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.2394 - accuracy: 0.9647 - val_loss: 5.4395 - val_accuracy: 0.4038\n",
            "Epoch 290/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.2385 - accuracy: 0.9588 - val_loss: 5.5325 - val_accuracy: 0.5000\n",
            "Epoch 291/500\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.2416 - accuracy: 0.9588 - val_loss: 5.6264 - val_accuracy: 0.5000\n",
            "Epoch 292/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.2354 - accuracy: 0.9588 - val_loss: 5.5653 - val_accuracy: 0.4808\n",
            "Epoch 293/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.2295 - accuracy: 0.9588 - val_loss: 5.5451 - val_accuracy: 0.4423\n",
            "Epoch 294/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.2302 - accuracy: 0.9647 - val_loss: 5.6210 - val_accuracy: 0.4808\n",
            "Epoch 295/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.2255 - accuracy: 0.9588 - val_loss: 5.6786 - val_accuracy: 0.5000\n",
            "Epoch 296/500\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.2287 - accuracy: 0.9588 - val_loss: 5.7018 - val_accuracy: 0.4615\n",
            "Epoch 297/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.2264 - accuracy: 0.9588 - val_loss: 5.6830 - val_accuracy: 0.4423\n",
            "Epoch 298/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.2256 - accuracy: 0.9706 - val_loss: 5.7634 - val_accuracy: 0.4808\n",
            "Epoch 299/500\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.2224 - accuracy: 0.9588 - val_loss: 5.8463 - val_accuracy: 0.5000\n",
            "Epoch 300/500\n",
            "3/3 [==============================] - 0s 118ms/step - loss: 0.2280 - accuracy: 0.9588 - val_loss: 5.8041 - val_accuracy: 0.4615\n",
            "Epoch 301/500\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.2216 - accuracy: 0.9588 - val_loss: 5.7457 - val_accuracy: 0.4423\n",
            "Epoch 302/500\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.2275 - accuracy: 0.9647 - val_loss: 5.8210 - val_accuracy: 0.4615\n",
            "Epoch 303/500\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.2171 - accuracy: 0.9647 - val_loss: 5.9338 - val_accuracy: 0.5000\n",
            "Epoch 304/500\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.2268 - accuracy: 0.9588 - val_loss: 5.9079 - val_accuracy: 0.4615\n",
            "Epoch 305/500\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.2226 - accuracy: 0.9588 - val_loss: 5.8476 - val_accuracy: 0.4231\n",
            "Epoch 306/500\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.2231 - accuracy: 0.9588 - val_loss: 5.8979 - val_accuracy: 0.4615\n",
            "Epoch 307/500\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.2352 - accuracy: 0.9471 - val_loss: 5.6645 - val_accuracy: 0.4808\n",
            "Epoch 308/500\n",
            "3/3 [==============================] - 0s 122ms/step - loss: 0.2791 - accuracy: 0.9471 - val_loss: 5.6636 - val_accuracy: 0.4231\n",
            "Epoch 309/500\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.2400 - accuracy: 0.9529 - val_loss: 5.4252 - val_accuracy: 0.3654\n",
            "Epoch 310/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.2923 - accuracy: 0.9294 - val_loss: 5.5534 - val_accuracy: 0.5000\n",
            "Epoch 311/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.2890 - accuracy: 0.9471 - val_loss: 5.5966 - val_accuracy: 0.5192\n",
            "Epoch 312/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.2378 - accuracy: 0.9529 - val_loss: 5.6608 - val_accuracy: 0.4038\n",
            "Epoch 313/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.2471 - accuracy: 0.9353 - val_loss: 5.7471 - val_accuracy: 0.4231\n",
            "Epoch 314/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.2227 - accuracy: 0.9706 - val_loss: 5.8370 - val_accuracy: 0.4615\n",
            "Epoch 315/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.2379 - accuracy: 0.9588 - val_loss: 5.6452 - val_accuracy: 0.4231\n",
            "Epoch 316/500\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.2339 - accuracy: 0.9353 - val_loss: 5.5395 - val_accuracy: 0.3654\n",
            "Epoch 317/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.2378 - accuracy: 0.9412 - val_loss: 5.5582 - val_accuracy: 0.4038\n",
            "Epoch 318/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.2196 - accuracy: 0.9588 - val_loss: 5.6003 - val_accuracy: 0.4423\n",
            "Epoch 319/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.2261 - accuracy: 0.9588 - val_loss: 5.5420 - val_accuracy: 0.4423\n",
            "Epoch 320/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.2107 - accuracy: 0.9588 - val_loss: 5.5195 - val_accuracy: 0.4231\n",
            "Epoch 321/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.2128 - accuracy: 0.9647 - val_loss: 5.5779 - val_accuracy: 0.4231\n",
            "Epoch 322/500\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.2112 - accuracy: 0.9647 - val_loss: 5.7173 - val_accuracy: 0.4231\n",
            "Epoch 323/500\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.2032 - accuracy: 0.9588 - val_loss: 5.8080 - val_accuracy: 0.4423\n",
            "Epoch 324/500\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.2094 - accuracy: 0.9588 - val_loss: 5.8156 - val_accuracy: 0.4231\n",
            "Epoch 325/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.2078 - accuracy: 0.9706 - val_loss: 5.8474 - val_accuracy: 0.4231\n",
            "Epoch 326/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.2066 - accuracy: 0.9647 - val_loss: 5.9406 - val_accuracy: 0.4423\n",
            "Epoch 327/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.2009 - accuracy: 0.9647 - val_loss: 5.9760 - val_accuracy: 0.4423\n",
            "Epoch 328/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.1974 - accuracy: 0.9588 - val_loss: 5.9452 - val_accuracy: 0.4423\n",
            "Epoch 329/500\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 0.1982 - accuracy: 0.9706 - val_loss: 5.9312 - val_accuracy: 0.4231\n",
            "Epoch 330/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.2001 - accuracy: 0.9706 - val_loss: 6.0242 - val_accuracy: 0.4423\n",
            "Epoch 331/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1968 - accuracy: 0.9647 - val_loss: 6.0551 - val_accuracy: 0.4423\n",
            "Epoch 332/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.1983 - accuracy: 0.9647 - val_loss: 6.1049 - val_accuracy: 0.4615\n",
            "Epoch 333/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.1925 - accuracy: 0.9706 - val_loss: 6.0888 - val_accuracy: 0.4231\n",
            "Epoch 334/500\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.1906 - accuracy: 0.9706 - val_loss: 6.1427 - val_accuracy: 0.4423\n",
            "Epoch 335/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.1890 - accuracy: 0.9706 - val_loss: 6.1943 - val_accuracy: 0.4615\n",
            "Epoch 336/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.1988 - accuracy: 0.9588 - val_loss: 6.2151 - val_accuracy: 0.4231\n",
            "Epoch 337/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.2033 - accuracy: 0.9588 - val_loss: 6.1137 - val_accuracy: 0.4038\n",
            "Epoch 338/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.2087 - accuracy: 0.9588 - val_loss: 6.2424 - val_accuracy: 0.4231\n",
            "Epoch 339/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.1924 - accuracy: 0.9647 - val_loss: 6.3484 - val_accuracy: 0.4423\n",
            "Epoch 340/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.2070 - accuracy: 0.9588 - val_loss: 6.2366 - val_accuracy: 0.4038\n",
            "Epoch 341/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.1917 - accuracy: 0.9647 - val_loss: 6.0900 - val_accuracy: 0.3846\n",
            "Epoch 342/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.2055 - accuracy: 0.9647 - val_loss: 6.0595 - val_accuracy: 0.3846\n",
            "Epoch 343/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.1949 - accuracy: 0.9647 - val_loss: 6.1414 - val_accuracy: 0.4423\n",
            "Epoch 344/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.1873 - accuracy: 0.9647 - val_loss: 6.1598 - val_accuracy: 0.4231\n",
            "Epoch 345/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1933 - accuracy: 0.9529 - val_loss: 6.1512 - val_accuracy: 0.4231\n",
            "Epoch 346/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.1921 - accuracy: 0.9471 - val_loss: 6.1799 - val_accuracy: 0.4231\n",
            "Epoch 347/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.1864 - accuracy: 0.9706 - val_loss: 6.1538 - val_accuracy: 0.4231\n",
            "Epoch 348/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.1838 - accuracy: 0.9647 - val_loss: 6.1452 - val_accuracy: 0.4231\n",
            "Epoch 349/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.1866 - accuracy: 0.9647 - val_loss: 6.1445 - val_accuracy: 0.4231\n",
            "Epoch 350/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.1918 - accuracy: 0.9647 - val_loss: 6.1865 - val_accuracy: 0.4423\n",
            "Epoch 351/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1777 - accuracy: 0.9706 - val_loss: 6.1613 - val_accuracy: 0.4038\n",
            "Epoch 352/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.1863 - accuracy: 0.9588 - val_loss: 6.1956 - val_accuracy: 0.3846\n",
            "Epoch 353/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1760 - accuracy: 0.9706 - val_loss: 6.2630 - val_accuracy: 0.4423\n",
            "Epoch 354/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.1805 - accuracy: 0.9706 - val_loss: 6.2521 - val_accuracy: 0.4423\n",
            "Epoch 355/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.1768 - accuracy: 0.9647 - val_loss: 6.1902 - val_accuracy: 0.3846\n",
            "Epoch 356/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.1781 - accuracy: 0.9706 - val_loss: 6.2195 - val_accuracy: 0.3846\n",
            "Epoch 357/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.1740 - accuracy: 0.9706 - val_loss: 6.2748 - val_accuracy: 0.4423\n",
            "Epoch 358/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.1837 - accuracy: 0.9647 - val_loss: 6.2807 - val_accuracy: 0.4231\n",
            "Epoch 359/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1753 - accuracy: 0.9706 - val_loss: 6.2271 - val_accuracy: 0.4038\n",
            "Epoch 360/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.1769 - accuracy: 0.9706 - val_loss: 6.2313 - val_accuracy: 0.3846\n",
            "Epoch 361/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.1750 - accuracy: 0.9706 - val_loss: 6.2233 - val_accuracy: 0.4423\n",
            "Epoch 362/500\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.1716 - accuracy: 0.9647 - val_loss: 6.2354 - val_accuracy: 0.4423\n",
            "Epoch 363/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.1733 - accuracy: 0.9647 - val_loss: 6.2200 - val_accuracy: 0.4231\n",
            "Epoch 364/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.1697 - accuracy: 0.9706 - val_loss: 6.2125 - val_accuracy: 0.4038\n",
            "Epoch 365/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1687 - accuracy: 0.9706 - val_loss: 6.2418 - val_accuracy: 0.4038\n",
            "Epoch 366/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.1699 - accuracy: 0.9706 - val_loss: 6.2569 - val_accuracy: 0.4038\n",
            "Epoch 367/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.1677 - accuracy: 0.9706 - val_loss: 6.2780 - val_accuracy: 0.4231\n",
            "Epoch 368/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.1682 - accuracy: 0.9647 - val_loss: 6.2663 - val_accuracy: 0.4038\n",
            "Epoch 369/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.1686 - accuracy: 0.9706 - val_loss: 6.2330 - val_accuracy: 0.3846\n",
            "Epoch 370/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.1694 - accuracy: 0.9706 - val_loss: 6.2325 - val_accuracy: 0.4038\n",
            "Epoch 371/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.1640 - accuracy: 0.9706 - val_loss: 6.2253 - val_accuracy: 0.4423\n",
            "Epoch 372/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.1697 - accuracy: 0.9706 - val_loss: 6.2435 - val_accuracy: 0.4423\n",
            "Epoch 373/500\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.1667 - accuracy: 0.9647 - val_loss: 6.2717 - val_accuracy: 0.4231\n",
            "Epoch 374/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.1722 - accuracy: 0.9706 - val_loss: 6.2256 - val_accuracy: 0.4038\n",
            "Epoch 375/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.1647 - accuracy: 0.9706 - val_loss: 6.2259 - val_accuracy: 0.4038\n",
            "Epoch 376/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.1737 - accuracy: 0.9412 - val_loss: 6.2370 - val_accuracy: 0.4423\n",
            "Epoch 377/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1712 - accuracy: 0.9706 - val_loss: 6.2317 - val_accuracy: 0.4423\n",
            "Epoch 378/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1657 - accuracy: 0.9706 - val_loss: 6.1897 - val_accuracy: 0.4038\n",
            "Epoch 379/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.1750 - accuracy: 0.9706 - val_loss: 6.2173 - val_accuracy: 0.4038\n",
            "Epoch 380/500\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.1722 - accuracy: 0.9706 - val_loss: 6.3364 - val_accuracy: 0.4423\n",
            "Epoch 381/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.1787 - accuracy: 0.9647 - val_loss: 6.1025 - val_accuracy: 0.3846\n",
            "Epoch 382/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.1657 - accuracy: 0.9706 - val_loss: 6.0231 - val_accuracy: 0.4231\n",
            "Epoch 383/500\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.1658 - accuracy: 0.9706 - val_loss: 6.0657 - val_accuracy: 0.4038\n",
            "Epoch 384/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.1724 - accuracy: 0.9765 - val_loss: 6.0911 - val_accuracy: 0.4038\n",
            "Epoch 385/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1653 - accuracy: 0.9647 - val_loss: 6.0700 - val_accuracy: 0.4231\n",
            "Epoch 386/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.1678 - accuracy: 0.9706 - val_loss: 6.0785 - val_accuracy: 0.4038\n",
            "Epoch 387/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.1609 - accuracy: 0.9706 - val_loss: 6.1089 - val_accuracy: 0.4038\n",
            "Epoch 388/500\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 0.1623 - accuracy: 0.9706 - val_loss: 6.1152 - val_accuracy: 0.4038\n",
            "Epoch 389/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1635 - accuracy: 0.9647 - val_loss: 6.0703 - val_accuracy: 0.4038\n",
            "Epoch 390/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.1612 - accuracy: 0.9706 - val_loss: 6.0774 - val_accuracy: 0.4038\n",
            "Epoch 391/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.1540 - accuracy: 0.9706 - val_loss: 6.1190 - val_accuracy: 0.4423\n",
            "Epoch 392/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1622 - accuracy: 0.9706 - val_loss: 6.1494 - val_accuracy: 0.4423\n",
            "Epoch 393/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.1585 - accuracy: 0.9765 - val_loss: 6.0918 - val_accuracy: 0.4038\n",
            "Epoch 394/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.1610 - accuracy: 0.9706 - val_loss: 6.0703 - val_accuracy: 0.3846\n",
            "Epoch 395/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.1570 - accuracy: 0.9706 - val_loss: 6.0696 - val_accuracy: 0.3846\n",
            "Epoch 396/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1529 - accuracy: 0.9706 - val_loss: 6.0973 - val_accuracy: 0.4423\n",
            "Epoch 397/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.1609 - accuracy: 0.9706 - val_loss: 6.0973 - val_accuracy: 0.4423\n",
            "Epoch 398/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.1539 - accuracy: 0.9706 - val_loss: 6.0640 - val_accuracy: 0.3846\n",
            "Epoch 399/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.1553 - accuracy: 0.9706 - val_loss: 6.0467 - val_accuracy: 0.4038\n",
            "Epoch 400/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.1528 - accuracy: 0.9706 - val_loss: 6.0490 - val_accuracy: 0.4038\n",
            "Epoch 401/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.1597 - accuracy: 0.9647 - val_loss: 6.0629 - val_accuracy: 0.4231\n",
            "Epoch 402/500\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.1546 - accuracy: 0.9647 - val_loss: 6.0219 - val_accuracy: 0.4038\n",
            "Epoch 403/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.1504 - accuracy: 0.9706 - val_loss: 6.0219 - val_accuracy: 0.4038\n",
            "Epoch 404/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.1534 - accuracy: 0.9706 - val_loss: 6.0126 - val_accuracy: 0.4231\n",
            "Epoch 405/500\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.1470 - accuracy: 0.9706 - val_loss: 6.0353 - val_accuracy: 0.4231\n",
            "Epoch 406/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.1566 - accuracy: 0.9706 - val_loss: 6.0336 - val_accuracy: 0.4231\n",
            "Epoch 407/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1534 - accuracy: 0.9647 - val_loss: 6.0061 - val_accuracy: 0.4038\n",
            "Epoch 408/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.1580 - accuracy: 0.9706 - val_loss: 5.9940 - val_accuracy: 0.4038\n",
            "Epoch 409/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.1546 - accuracy: 0.9706 - val_loss: 6.0156 - val_accuracy: 0.4423\n",
            "Epoch 410/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.1435 - accuracy: 0.9706 - val_loss: 6.0297 - val_accuracy: 0.4615\n",
            "Epoch 411/500\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.1604 - accuracy: 0.9706 - val_loss: 5.9977 - val_accuracy: 0.4423\n",
            "Epoch 412/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.1582 - accuracy: 0.9706 - val_loss: 5.9551 - val_accuracy: 0.4038\n",
            "Epoch 413/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1539 - accuracy: 0.9706 - val_loss: 6.0575 - val_accuracy: 0.4423\n",
            "Epoch 414/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.1578 - accuracy: 0.9765 - val_loss: 6.0745 - val_accuracy: 0.4423\n",
            "Epoch 415/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1514 - accuracy: 0.9706 - val_loss: 5.9812 - val_accuracy: 0.4038\n",
            "Epoch 416/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.1487 - accuracy: 0.9706 - val_loss: 5.9698 - val_accuracy: 0.4038\n",
            "Epoch 417/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.1513 - accuracy: 0.9706 - val_loss: 5.9878 - val_accuracy: 0.4231\n",
            "Epoch 418/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.1495 - accuracy: 0.9706 - val_loss: 6.0225 - val_accuracy: 0.4423\n",
            "Epoch 419/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.1528 - accuracy: 0.9706 - val_loss: 5.9690 - val_accuracy: 0.4038\n",
            "Epoch 420/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1526 - accuracy: 0.9706 - val_loss: 5.9084 - val_accuracy: 0.4231\n",
            "Epoch 421/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.1544 - accuracy: 0.9706 - val_loss: 5.9519 - val_accuracy: 0.4231\n",
            "Epoch 422/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.1480 - accuracy: 0.9765 - val_loss: 5.9818 - val_accuracy: 0.4038\n",
            "Epoch 423/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1442 - accuracy: 0.9706 - val_loss: 5.9749 - val_accuracy: 0.4038\n",
            "Epoch 424/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1516 - accuracy: 0.9588 - val_loss: 5.9479 - val_accuracy: 0.4038\n",
            "Epoch 425/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.1423 - accuracy: 0.9706 - val_loss: 5.9431 - val_accuracy: 0.4038\n",
            "Epoch 426/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.1502 - accuracy: 0.9706 - val_loss: 5.9497 - val_accuracy: 0.4231\n",
            "Epoch 427/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.1477 - accuracy: 0.9765 - val_loss: 5.9324 - val_accuracy: 0.4038\n",
            "Epoch 428/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.1395 - accuracy: 0.9706 - val_loss: 5.9327 - val_accuracy: 0.4231\n",
            "Epoch 429/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.1457 - accuracy: 0.9706 - val_loss: 5.9353 - val_accuracy: 0.4231\n",
            "Epoch 430/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.1469 - accuracy: 0.9706 - val_loss: 5.9840 - val_accuracy: 0.4423\n",
            "Epoch 431/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.1422 - accuracy: 0.9706 - val_loss: 5.9519 - val_accuracy: 0.4231\n",
            "Epoch 432/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.1465 - accuracy: 0.9706 - val_loss: 5.9272 - val_accuracy: 0.4038\n",
            "Epoch 433/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.1435 - accuracy: 0.9706 - val_loss: 5.9385 - val_accuracy: 0.4038\n",
            "Epoch 434/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.1410 - accuracy: 0.9765 - val_loss: 5.9374 - val_accuracy: 0.4038\n",
            "Epoch 435/500\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.1502 - accuracy: 0.9706 - val_loss: 5.9087 - val_accuracy: 0.4038\n",
            "Epoch 436/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.1383 - accuracy: 0.9706 - val_loss: 5.8962 - val_accuracy: 0.4038\n",
            "Epoch 437/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1374 - accuracy: 0.9765 - val_loss: 5.8866 - val_accuracy: 0.4231\n",
            "Epoch 438/500\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.1473 - accuracy: 0.9765 - val_loss: 5.8843 - val_accuracy: 0.4231\n",
            "Epoch 439/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1403 - accuracy: 0.9765 - val_loss: 5.8915 - val_accuracy: 0.4615\n",
            "Epoch 440/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 0.1621 - accuracy: 0.9412 - val_loss: 5.8618 - val_accuracy: 0.4615\n",
            "Epoch 441/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1512 - accuracy: 0.9706 - val_loss: 5.8243 - val_accuracy: 0.4423\n",
            "Epoch 442/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.1538 - accuracy: 0.9706 - val_loss: 5.7628 - val_accuracy: 0.4231\n",
            "Epoch 443/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.1441 - accuracy: 0.9765 - val_loss: 5.8517 - val_accuracy: 0.4615\n",
            "Epoch 444/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.1487 - accuracy: 0.9706 - val_loss: 5.8064 - val_accuracy: 0.4231\n",
            "Epoch 445/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1391 - accuracy: 0.9706 - val_loss: 5.7474 - val_accuracy: 0.4038\n",
            "Epoch 446/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.1462 - accuracy: 0.9471 - val_loss: 5.7559 - val_accuracy: 0.4231\n",
            "Epoch 447/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.1382 - accuracy: 0.9765 - val_loss: 5.7819 - val_accuracy: 0.4231\n",
            "Epoch 448/500\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 0.1404 - accuracy: 0.9765 - val_loss: 5.7896 - val_accuracy: 0.4231\n",
            "Epoch 449/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.1357 - accuracy: 0.9706 - val_loss: 5.7496 - val_accuracy: 0.4038\n",
            "Epoch 450/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.1327 - accuracy: 0.9706 - val_loss: 5.7299 - val_accuracy: 0.3846\n",
            "Epoch 451/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.1461 - accuracy: 0.9706 - val_loss: 5.7341 - val_accuracy: 0.4038\n",
            "Epoch 452/500\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.1322 - accuracy: 0.9706 - val_loss: 5.8127 - val_accuracy: 0.4231\n",
            "Epoch 453/500\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.1509 - accuracy: 0.9647 - val_loss: 5.7842 - val_accuracy: 0.4423\n",
            "Epoch 454/500\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.1301 - accuracy: 0.9765 - val_loss: 5.7159 - val_accuracy: 0.3846\n",
            "Epoch 455/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.1462 - accuracy: 0.9706 - val_loss: 5.7420 - val_accuracy: 0.4038\n",
            "Epoch 456/500\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.1433 - accuracy: 0.9588 - val_loss: 5.7982 - val_accuracy: 0.4231\n",
            "Epoch 457/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.1351 - accuracy: 0.9765 - val_loss: 5.7581 - val_accuracy: 0.4615\n",
            "Epoch 458/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1335 - accuracy: 0.9706 - val_loss: 5.7002 - val_accuracy: 0.4231\n",
            "Epoch 459/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.1345 - accuracy: 0.9706 - val_loss: 5.6702 - val_accuracy: 0.3846\n",
            "Epoch 460/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.1311 - accuracy: 0.9706 - val_loss: 5.6767 - val_accuracy: 0.4038\n",
            "Epoch 461/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.1351 - accuracy: 0.9706 - val_loss: 5.6940 - val_accuracy: 0.4038\n",
            "Epoch 462/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1367 - accuracy: 0.9706 - val_loss: 5.7317 - val_accuracy: 0.4038\n",
            "Epoch 463/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.1369 - accuracy: 0.9706 - val_loss: 5.7171 - val_accuracy: 0.4038\n",
            "Epoch 464/500\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 0.1324 - accuracy: 0.9765 - val_loss: 5.6452 - val_accuracy: 0.3846\n",
            "Epoch 465/500\n",
            "3/3 [==============================] - 0s 107ms/step - loss: 0.1384 - accuracy: 0.9706 - val_loss: 5.5710 - val_accuracy: 0.4038\n",
            "Epoch 466/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1371 - accuracy: 0.9706 - val_loss: 5.6303 - val_accuracy: 0.4038\n",
            "Epoch 467/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.1346 - accuracy: 0.9765 - val_loss: 5.6779 - val_accuracy: 0.4038\n",
            "Epoch 468/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.1348 - accuracy: 0.9765 - val_loss: 5.7321 - val_accuracy: 0.4038\n",
            "Epoch 469/500\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.1395 - accuracy: 0.9765 - val_loss: 5.7959 - val_accuracy: 0.4038\n",
            "Epoch 470/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 0.1411 - accuracy: 0.9706 - val_loss: 5.7831 - val_accuracy: 0.4231\n",
            "Epoch 471/500\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.1285 - accuracy: 0.9765 - val_loss: 5.7274 - val_accuracy: 0.4423\n",
            "Epoch 472/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.1347 - accuracy: 0.9706 - val_loss: 5.6772 - val_accuracy: 0.3846\n",
            "Epoch 473/500\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.1379 - accuracy: 0.9706 - val_loss: 5.6732 - val_accuracy: 0.4038\n",
            "Epoch 474/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.1378 - accuracy: 0.9647 - val_loss: 5.7453 - val_accuracy: 0.4231\n",
            "Epoch 475/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.1361 - accuracy: 0.9706 - val_loss: 5.6522 - val_accuracy: 0.4038\n",
            "Epoch 476/500\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.1265 - accuracy: 0.9765 - val_loss: 5.6193 - val_accuracy: 0.4038\n",
            "Epoch 477/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.1433 - accuracy: 0.9529 - val_loss: 5.6305 - val_accuracy: 0.4038\n",
            "Epoch 478/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.1309 - accuracy: 0.9765 - val_loss: 5.6988 - val_accuracy: 0.4423\n",
            "Epoch 479/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.1510 - accuracy: 0.9588 - val_loss: 5.5863 - val_accuracy: 0.4038\n",
            "Epoch 480/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.1339 - accuracy: 0.9706 - val_loss: 5.5302 - val_accuracy: 0.4038\n",
            "Epoch 481/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.1421 - accuracy: 0.9706 - val_loss: 5.5479 - val_accuracy: 0.4038\n",
            "Epoch 482/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.1326 - accuracy: 0.9706 - val_loss: 5.7016 - val_accuracy: 0.4231\n",
            "Epoch 483/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.1366 - accuracy: 0.9706 - val_loss: 5.6540 - val_accuracy: 0.4231\n",
            "Epoch 484/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.1282 - accuracy: 0.9765 - val_loss: 5.6263 - val_accuracy: 0.3654\n",
            "Epoch 485/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.1337 - accuracy: 0.9765 - val_loss: 5.6168 - val_accuracy: 0.3654\n",
            "Epoch 486/500\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.1257 - accuracy: 0.9765 - val_loss: 5.6725 - val_accuracy: 0.3846\n",
            "Epoch 487/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1352 - accuracy: 0.9706 - val_loss: 5.7952 - val_accuracy: 0.4231\n",
            "Epoch 488/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.1324 - accuracy: 0.9706 - val_loss: 5.8454 - val_accuracy: 0.4231\n",
            "Epoch 489/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.1304 - accuracy: 0.9706 - val_loss: 5.7043 - val_accuracy: 0.4231\n",
            "Epoch 490/500\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.1315 - accuracy: 0.9706 - val_loss: 5.6534 - val_accuracy: 0.3654\n",
            "Epoch 491/500\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.1257 - accuracy: 0.9706 - val_loss: 5.5891 - val_accuracy: 0.4038\n",
            "Epoch 492/500\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.1229 - accuracy: 0.9706 - val_loss: 5.5655 - val_accuracy: 0.4038\n",
            "Epoch 493/500\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.1285 - accuracy: 0.9765 - val_loss: 5.5780 - val_accuracy: 0.4231\n",
            "Epoch 494/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.1262 - accuracy: 0.9765 - val_loss: 5.6208 - val_accuracy: 0.4038\n",
            "Epoch 495/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.1292 - accuracy: 0.9706 - val_loss: 5.6380 - val_accuracy: 0.3846\n",
            "Epoch 496/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.1283 - accuracy: 0.9706 - val_loss: 5.6731 - val_accuracy: 0.4038\n",
            "Epoch 497/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.1215 - accuracy: 0.9765 - val_loss: 5.7204 - val_accuracy: 0.3846\n",
            "Epoch 498/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.1241 - accuracy: 0.9765 - val_loss: 5.7500 - val_accuracy: 0.4231\n",
            "Epoch 499/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 0.1252 - accuracy: 0.9765 - val_loss: 5.7256 - val_accuracy: 0.4038\n",
            "Epoch 500/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.1214 - accuracy: 0.9706 - val_loss: 5.7038 - val_accuracy: 0.3654\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.703834056854248, 0.36538460850715637]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "### 8(35) 10(33) 12(SoTA, 1e-3) (8,12)\n",
        "## (42, 0) (12, 100) (12, 12) (42, 100)\n",
        "\n",
        "RANDOM = 42\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(RANDOM)\n",
        "import random\n",
        "random.seed(RANDOM)\n",
        "import numpy as np\n",
        "np.random.seed(RANDOM)\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    seed(RANDOM)\n",
        "    tf.random.set_seed(RANDOM)\n",
        "    np.random.seed(RANDOM)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.Dense(256, activation=\"relu\", kernel_regularizer='l2')(x)    \n",
        "    x = layers.Dense(128, activation=\"relu\", kernel_regularizer='l2')(x)\n",
        "    x = layers.Dense(inputs.shape[-1])(x)\n",
        "    return x + res\n",
        "\n",
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "    n_classes = 3\n",
        "):\n",
        "    tf.random.set_seed(RANDOM)\n",
        "    seed(RANDOM)\n",
        "    np.random.seed(RANDOM)\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "\n",
        "    x = layers.Dense(128, activation=\"relu\", kernel_regularizer='l2')(x)\n",
        "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "RANDOM = 0\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(RANDOM)\n",
        "import random\n",
        "random.seed(RANDOM)\n",
        "import numpy as np\n",
        "np.random.seed(RANDOM)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "seed(RANDOM)\n",
        "tf.random.set_seed(RANDOM)\n",
        "np.random.seed(RANDOM)\n",
        "model = build_model(\n",
        "    input_shape,\n",
        "    head_size=5,\n",
        "    num_heads=2,\n",
        "    ff_dim=3,\n",
        "    num_transformer_blocks=12,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0,\n",
        "    dropout=0,\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "history = model.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=1000, restore_best_weights=True)]\n",
        "\n",
        "curve = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=500,\n",
        "    batch_size=64,\n",
        ")\n",
        "\n",
        "model.evaluate(X_test, y_test, verbose=10)    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "S9BQwo-AtzpZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9BQwo-AtzpZ",
        "outputId": "1193c7b1-eb30-46dc-80ce-91cc8532276b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.7201743125915527, 0.44999998807907104]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "model.evaluate(X_val, y_val, verbose=10)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "IhK1buKHAGSs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhK1buKHAGSs",
        "outputId": "9efe4a60-75fa-43ff-a5c8-f613ea5cbe03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7843137254901961\n",
            "0.31092436974789917\n",
            "0.33986928104575165\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import roc_curve, confusion_matrix, roc_auc_score\n",
        "import seaborn as sns\n",
        "# import torch.nn.functional as nnf\n",
        "# model.eval()\n",
        "pred = model(X_test)\n",
        "# pred = bestcnn(X_test)\n",
        "pred_prob = tf.nn.softmax(pred, axis=1)\n",
        "\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}      \n",
        "n_class = 3\n",
        "\n",
        "for i in range(n_class):    \n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)\n",
        "\n",
        "a = []\n",
        "classes = [0,1,2]\n",
        "for i in range(len(classes)):\n",
        "    # Gets the class\n",
        "    c = classes[i]\n",
        "    \n",
        "    # Prepares an auxiliar dataframe to help with the plots\n",
        "    df_aux = pd.DataFrame()\n",
        "    df_aux['class'] = [1 if y == c else 0 for y in y_test]\n",
        "    df_aux['prob'] = pred_prob[:, i]\n",
        "    df_aux = df_aux.reset_index(drop = True)\n",
        "\n",
        "    \n",
        "    # Calculates the ROC AUC OvR\n",
        "    a.append(round(roc_auc_score(df_aux['class'], df_aux['prob']),3))\n",
        "    print(roc_auc_score(df_aux['class'], df_aux['prob']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AZwNCYEy1UaM"
      },
      "id": "AZwNCYEy1UaM",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "8ml6D_iUGVcJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "8ml6D_iUGVcJ",
        "outputId": "13f28c6e-d911-478a-da31-92394e114d2f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set_style(\"white\")\n",
        "plt.plot(fpr[0], tpr[0], linestyle='-',color='orange', label=f'class-0 {a[0]}')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='-',color='green', label=f'class-1 {a[1]}')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='-',color='red', label=f'class-2 {a[2]}')\n",
        "# plt.plot(fpr[3], tpr[3], linestyle='-',color='blue', label=f'class-3 {a[3]}')\n",
        "plt.title('Multiclass ROC curve using Transformer',fontsize=15)\n",
        "plt.xlabel('False Positive Rate',fontsize=15)\n",
        "plt.ylabel('True Positive rate',fontsize=15)\n",
        "plt.tick_params(labelsize=15)\n",
        "sns.set(rc={'figure.figsize':(6,6)})\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='grey', label='Random guess')\n",
        "# plt.legend(loc='best',fontsize=10)\n",
        "plt.legend(loc=\"lower right\", fontsize=10);\n",
        "plt.savefig(\"transformer.pdf\",dpi=300);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "WQzWTNLFqDYb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQzWTNLFqDYb",
        "outputId": "5d5dcc68-7437-42e0-e7bc-3cc867efddd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.00      0.00      0.00         1\n",
            "     class 1       0.00      0.00      0.00        17\n",
            "     class 2       0.65      1.00      0.79        34\n",
            "\n",
            "    accuracy                           0.65        52\n",
            "   macro avg       0.22      0.33      0.26        52\n",
            "weighted avg       0.43      0.65      0.52        52\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_true = y_test\n",
        "y_pred = np.argmax(model(X_test), axis=1)\n",
        "target_names = ['class 0', 'class 1', 'class 2']\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6i6ri_ckLiJ",
      "metadata": {
        "id": "f6i6ri_ckLiJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IjhQzALexdxV",
      "metadata": {
        "id": "IjhQzALexdxV"
      },
      "outputs": [],
      "source": [
        "# bestmodel = keras.models.load_model(\"drive/MyDrive/model_transformer_final\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e80531b8",
      "metadata": {
        "id": "e80531b8"
      },
      "outputs": [],
      "source": [
        "def extract(arr):\n",
        "  out = []\n",
        "  for weights in arr:\n",
        "    max_weights = np.argmax(weights)\n",
        "    out.append(max_weights)\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e783ouAv1EPM",
      "metadata": {
        "id": "e783ouAv1EPM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "sns.set_style(\"white\")\n",
        "plt.plot(curve.history['loss'][0:1000])\n",
        "plt.plot(curve.history['val_loss'][0:1000])\n",
        "plt.title(\"Loss Curve of Transformer-based Model\",fontsize=15)\n",
        "plt.ylabel('loss',fontsize=15)\n",
        "plt.xlabel('epoch',fontsize=15)\n",
        "plt.tick_params(labelsize=15)\n",
        "sns.set(rc={'figure.figsize':(6,6)})\n",
        "# plt.ylim(0.8, 2.2)\n",
        "plt.legend(['train', \"validation\"], loc='upper right')\n",
        "plt.savefig(\"transformer_loss.pdf\",dpi=300);\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0_QFfeHhOBuw",
      "metadata": {
        "id": "0_QFfeHhOBuw"
      },
      "outputs": [],
      "source": [
        "plt.plot(curve.history['accuracy'])\n",
        "plt.plot(curve.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(curve.history['loss'][:500])\n",
        "plt.plot(curve.history['val_loss'][:500])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81416fc8",
      "metadata": {
        "id": "81416fc8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9b5193a6",
        "OMKsNJVxJZMF",
        "842c0fc9",
        "e023e5aa"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "3f2099620ee754a799ea2efde844907f572fbdcd9ece8dce41bdce43c5c037bd"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}