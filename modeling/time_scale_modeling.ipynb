{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f399f969",
      "metadata": {
        "id": "f399f969"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "842c0fc9",
      "metadata": {
        "id": "842c0fc9"
      },
      "source": [
        "### Train & Val Split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(\"label.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "53eyKb4QrcL8",
        "outputId": "a7ad78f7-deed-48f2-9f1c-02fae2a269ed"
      },
      "id": "53eyKb4QrcL8",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            ResponseId  label\n",
              "0    R_0DNcJqALC3UG9Db      2\n",
              "1    R_0HbfbDtnkUIRbvH      3\n",
              "2    R_0cVkkjfJMy8V6P7      3\n",
              "3    R_10OZ3Fgz4TJo8Vx      2\n",
              "4    R_11WSDnMOeKXakQH      2\n",
              "..                 ...    ...\n",
              "237  R_xlnJEPOkxxHspJT      2\n",
              "238  R_yNJaIImQ7EY3J5v      2\n",
              "239  R_yqY6xKo4dUHco6d      3\n",
              "240  R_yrKr3inrXDL5Lvb      3\n",
              "241  R_z6DSJfELElPHABP      3\n",
              "\n",
              "[242 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1615c88d-a14c-487b-9444-cfd7412b83ae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ResponseId</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>R_0DNcJqALC3UG9Db</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>R_0HbfbDtnkUIRbvH</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>R_0cVkkjfJMy8V6P7</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>R_10OZ3Fgz4TJo8Vx</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>R_11WSDnMOeKXakQH</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>R_xlnJEPOkxxHspJT</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>238</th>\n",
              "      <td>R_yNJaIImQ7EY3J5v</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>R_yqY6xKo4dUHco6d</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>R_yrKr3inrXDL5Lvb</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>R_z6DSJfELElPHABP</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>242 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1615c88d-a14c-487b-9444-cfd7412b83ae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1615c88d-a14c-487b-9444-cfd7412b83ae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1615c88d-a14c-487b-9444-cfd7412b83ae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "61ced19f",
      "metadata": {
        "id": "61ced19f"
      },
      "outputs": [],
      "source": [
        "tensor = np.load(\"embedding_new.npy\")\n",
        "# tensor = np.load(\"onehot.npy\")\n",
        "y = pd.read_csv(\"label.csv\")[\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1ef96c50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ef96c50",
        "outputId": "4d3ab436-f4fc-4188-9313-fb805f209fe2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "y -= 1\n",
        "y.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7bf84c3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bf84c3e",
        "outputId": "c4f8a9bd-f1e9-4aba-f045-952ddd18dfec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(170, 75, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "X_train, y_train = tensor[:170], y[:170]\n",
        "X_val, y_val = tensor[170:190], y[170:190]\n",
        "X_test, y_test = tensor[190:], y[190:]\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "LLzZtbFlHr3R",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLzZtbFlHr3R",
        "outputId": "7a8cc5fe-8bfb-491d-e3dd-b5c814b387c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.3279645,  1.1193714,  1.3651992,  1.4757956,  1.2556211,\n",
              "       -0.576183 , -0.520399 ,  1.6278158], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "X_train[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oI1-HCaUYsiV",
      "metadata": {
        "id": "oI1-HCaUYsiV"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "Qkar158XyLwV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qkar158XyLwV",
        "outputId": "5c04b1f5-f23e-43ea-8b65-5bc32c4de081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "print(tf.test.gpu_device_name())\n",
        "from numpy.random import seed\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "JGtCxoTQOkch",
      "metadata": {
        "id": "JGtCxoTQOkch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df8381e-ebcc-440a-ec39-626bbac521b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 75, 8)]      0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization_24 (LayerN  (None, 75, 8)       16          ['input_3[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_24 (Multi  (None, 75, 8)       358         ['layer_normalization_24[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " dense_76 (Dense)               (None, 75, 256)      2304        ['multi_head_attention_24[0][0]']\n",
            "                                                                                                  \n",
            " dense_77 (Dense)               (None, 75, 128)      32896       ['dense_76[0][0]']               \n",
            "                                                                                                  \n",
            " dense_78 (Dense)               (None, 75, 8)        1032        ['dense_77[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_48 (TFOpL  (None, 75, 8)       0           ['multi_head_attention_24[0][0]',\n",
            " ambda)                                                           'input_3[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_49 (TFOpL  (None, 75, 8)       0           ['dense_78[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_48[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_25 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_49[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_25 (Multi  (None, 75, 8)       358         ['layer_normalization_25[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " dense_79 (Dense)               (None, 75, 256)      2304        ['multi_head_attention_25[0][0]']\n",
            "                                                                                                  \n",
            " dense_80 (Dense)               (None, 75, 128)      32896       ['dense_79[0][0]']               \n",
            "                                                                                                  \n",
            " dense_81 (Dense)               (None, 75, 8)        1032        ['dense_80[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_50 (TFOpL  (None, 75, 8)       0           ['multi_head_attention_25[0][0]',\n",
            " ambda)                                                           'tf.__operators__.add_49[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_51 (TFOpL  (None, 75, 8)       0           ['dense_81[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_50[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_26 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_51[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_26 (Multi  (None, 75, 8)       358         ['layer_normalization_26[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " dense_82 (Dense)               (None, 75, 256)      2304        ['multi_head_attention_26[0][0]']\n",
            "                                                                                                  \n",
            " dense_83 (Dense)               (None, 75, 128)      32896       ['dense_82[0][0]']               \n",
            "                                                                                                  \n",
            " dense_84 (Dense)               (None, 75, 8)        1032        ['dense_83[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_52 (TFOpL  (None, 75, 8)       0           ['multi_head_attention_26[0][0]',\n",
            " ambda)                                                           'tf.__operators__.add_51[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_53 (TFOpL  (None, 75, 8)       0           ['dense_84[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_52[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_27 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_53[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_27 (Multi  (None, 75, 8)       358         ['layer_normalization_27[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " dense_85 (Dense)               (None, 75, 256)      2304        ['multi_head_attention_27[0][0]']\n",
            "                                                                                                  \n",
            " dense_86 (Dense)               (None, 75, 128)      32896       ['dense_85[0][0]']               \n",
            "                                                                                                  \n",
            " dense_87 (Dense)               (None, 75, 8)        1032        ['dense_86[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_54 (TFOpL  (None, 75, 8)       0           ['multi_head_attention_27[0][0]',\n",
            " ambda)                                                           'tf.__operators__.add_53[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_55 (TFOpL  (None, 75, 8)       0           ['dense_87[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_54[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_28 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_55[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_28 (Multi  (None, 75, 8)       358         ['layer_normalization_28[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " dense_88 (Dense)               (None, 75, 256)      2304        ['multi_head_attention_28[0][0]']\n",
            "                                                                                                  \n",
            " dense_89 (Dense)               (None, 75, 128)      32896       ['dense_88[0][0]']               \n",
            "                                                                                                  \n",
            " dense_90 (Dense)               (None, 75, 8)        1032        ['dense_89[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_56 (TFOpL  (None, 75, 8)       0           ['multi_head_attention_28[0][0]',\n",
            " ambda)                                                           'tf.__operators__.add_55[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_57 (TFOpL  (None, 75, 8)       0           ['dense_90[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_56[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_29 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_57[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_29 (Multi  (None, 75, 8)       358         ['layer_normalization_29[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " dense_91 (Dense)               (None, 75, 256)      2304        ['multi_head_attention_29[0][0]']\n",
            "                                                                                                  \n",
            " dense_92 (Dense)               (None, 75, 128)      32896       ['dense_91[0][0]']               \n",
            "                                                                                                  \n",
            " dense_93 (Dense)               (None, 75, 8)        1032        ['dense_92[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_58 (TFOpL  (None, 75, 8)       0           ['multi_head_attention_29[0][0]',\n",
            " ambda)                                                           'tf.__operators__.add_57[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_59 (TFOpL  (None, 75, 8)       0           ['dense_93[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_58[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_30 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_59[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_30 (Multi  (None, 75, 8)       358         ['layer_normalization_30[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " dense_94 (Dense)               (None, 75, 256)      2304        ['multi_head_attention_30[0][0]']\n",
            "                                                                                                  \n",
            " dense_95 (Dense)               (None, 75, 128)      32896       ['dense_94[0][0]']               \n",
            "                                                                                                  \n",
            " dense_96 (Dense)               (None, 75, 8)        1032        ['dense_95[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_60 (TFOpL  (None, 75, 8)       0           ['multi_head_attention_30[0][0]',\n",
            " ambda)                                                           'tf.__operators__.add_59[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_61 (TFOpL  (None, 75, 8)       0           ['dense_96[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_60[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_31 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_61[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_31 (Multi  (None, 75, 8)       358         ['layer_normalization_31[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " dense_97 (Dense)               (None, 75, 256)      2304        ['multi_head_attention_31[0][0]']\n",
            "                                                                                                  \n",
            " dense_98 (Dense)               (None, 75, 128)      32896       ['dense_97[0][0]']               \n",
            "                                                                                                  \n",
            " dense_99 (Dense)               (None, 75, 8)        1032        ['dense_98[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_62 (TFOpL  (None, 75, 8)       0           ['multi_head_attention_31[0][0]',\n",
            " ambda)                                                           'tf.__operators__.add_61[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_63 (TFOpL  (None, 75, 8)       0           ['dense_99[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_62[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_32 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_63[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_32 (Multi  (None, 75, 8)       358         ['layer_normalization_32[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " dense_100 (Dense)              (None, 75, 256)      2304        ['multi_head_attention_32[0][0]']\n",
            "                                                                                                  \n",
            " dense_101 (Dense)              (None, 75, 128)      32896       ['dense_100[0][0]']              \n",
            "                                                                                                  \n",
            " dense_102 (Dense)              (None, 75, 8)        1032        ['dense_101[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_64 (TFOpL  (None, 75, 8)       0           ['multi_head_attention_32[0][0]',\n",
            " ambda)                                                           'tf.__operators__.add_63[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_65 (TFOpL  (None, 75, 8)       0           ['dense_102[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_64[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_33 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_65[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_33 (Multi  (None, 75, 8)       358         ['layer_normalization_33[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " dense_103 (Dense)              (None, 75, 256)      2304        ['multi_head_attention_33[0][0]']\n",
            "                                                                                                  \n",
            " dense_104 (Dense)              (None, 75, 128)      32896       ['dense_103[0][0]']              \n",
            "                                                                                                  \n",
            " dense_105 (Dense)              (None, 75, 8)        1032        ['dense_104[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_66 (TFOpL  (None, 75, 8)       0           ['multi_head_attention_33[0][0]',\n",
            " ambda)                                                           'tf.__operators__.add_65[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_67 (TFOpL  (None, 75, 8)       0           ['dense_105[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_66[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_34 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_67[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_34 (Multi  (None, 75, 8)       358         ['layer_normalization_34[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " dense_106 (Dense)              (None, 75, 256)      2304        ['multi_head_attention_34[0][0]']\n",
            "                                                                                                  \n",
            " dense_107 (Dense)              (None, 75, 128)      32896       ['dense_106[0][0]']              \n",
            "                                                                                                  \n",
            " dense_108 (Dense)              (None, 75, 8)        1032        ['dense_107[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_68 (TFOpL  (None, 75, 8)       0           ['multi_head_attention_34[0][0]',\n",
            " ambda)                                                           'tf.__operators__.add_67[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_69 (TFOpL  (None, 75, 8)       0           ['dense_108[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_68[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_35 (LayerN  (None, 75, 8)       16          ['tf.__operators__.add_69[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_35 (Multi  (None, 75, 8)       358         ['layer_normalization_35[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " dense_109 (Dense)              (None, 75, 256)      2304        ['multi_head_attention_35[0][0]']\n",
            "                                                                                                  \n",
            " dense_110 (Dense)              (None, 75, 128)      32896       ['dense_109[0][0]']              \n",
            "                                                                                                  \n",
            " dense_111 (Dense)              (None, 75, 8)        1032        ['dense_110[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_70 (TFOpL  (None, 75, 8)       0           ['multi_head_attention_35[0][0]',\n",
            " ambda)                                                           'tf.__operators__.add_69[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_71 (TFOpL  (None, 75, 8)       0           ['dense_111[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_70[0][0]']\n",
            "                                                                                                  \n",
            " global_average_pooling1d_2 (Gl  (None, 75)          0           ['tf.__operators__.add_71[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_112 (Dense)              (None, 128)          9728        ['global_average_pooling1d_2[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_113 (Dense)              (None, 3)            387         ['dense_112[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 449,387\n",
            "Trainable params: 449,387\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "3/3 [==============================] - 13s 842ms/step - loss: 93.1824 - accuracy: 0.3353 - val_loss: 93.0096 - val_accuracy: 0.2885\n",
            "Epoch 2/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 93.0155 - accuracy: 0.3176 - val_loss: 92.8382 - val_accuracy: 0.3077\n",
            "Epoch 3/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 92.8420 - accuracy: 0.4471 - val_loss: 92.6497 - val_accuracy: 0.5385\n",
            "Epoch 4/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 92.6567 - accuracy: 0.5471 - val_loss: 92.4529 - val_accuracy: 0.6154\n",
            "Epoch 5/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 92.4680 - accuracy: 0.7000 - val_loss: 92.2666 - val_accuracy: 0.6538\n",
            "Epoch 6/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 92.2810 - accuracy: 0.7059 - val_loss: 92.1155 - val_accuracy: 0.6538\n",
            "Epoch 7/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 92.1236 - accuracy: 0.7059 - val_loss: 91.9999 - val_accuracy: 0.6538\n",
            "Epoch 8/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 91.9970 - accuracy: 0.7059 - val_loss: 91.8916 - val_accuracy: 0.6538\n",
            "Epoch 9/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 91.8636 - accuracy: 0.7059 - val_loss: 91.7640 - val_accuracy: 0.6538\n",
            "Epoch 10/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 91.7215 - accuracy: 0.7059 - val_loss: 91.6338 - val_accuracy: 0.6538\n",
            "Epoch 11/500\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 91.5824 - accuracy: 0.7059 - val_loss: 91.5096 - val_accuracy: 0.6538\n",
            "Epoch 12/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 91.4530 - accuracy: 0.7059 - val_loss: 91.3847 - val_accuracy: 0.6538\n",
            "Epoch 13/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 91.3241 - accuracy: 0.7059 - val_loss: 91.2565 - val_accuracy: 0.6538\n",
            "Epoch 14/500\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 91.1920 - accuracy: 0.7059 - val_loss: 91.1274 - val_accuracy: 0.6538\n",
            "Epoch 15/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 91.0603 - accuracy: 0.7059 - val_loss: 90.9987 - val_accuracy: 0.6538\n",
            "Epoch 16/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 90.9302 - accuracy: 0.7059 - val_loss: 90.8702 - val_accuracy: 0.6538\n",
            "Epoch 17/500\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 90.8005 - accuracy: 0.7059 - val_loss: 90.7379 - val_accuracy: 0.6538\n",
            "Epoch 18/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 90.6701 - accuracy: 0.7059 - val_loss: 90.6056 - val_accuracy: 0.6538\n",
            "Epoch 19/500\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 90.5406 - accuracy: 0.7059 - val_loss: 90.4738 - val_accuracy: 0.6538\n",
            "Epoch 20/500\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 90.4109 - accuracy: 0.7059 - val_loss: 90.3439 - val_accuracy: 0.6538\n",
            "Epoch 21/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 90.2826 - accuracy: 0.7059 - val_loss: 90.2135 - val_accuracy: 0.6538\n",
            "Epoch 22/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 90.1532 - accuracy: 0.7059 - val_loss: 90.0836 - val_accuracy: 0.6538\n",
            "Epoch 23/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 90.0248 - accuracy: 0.7059 - val_loss: 89.9551 - val_accuracy: 0.6538\n",
            "Epoch 24/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 89.8968 - accuracy: 0.7059 - val_loss: 89.8268 - val_accuracy: 0.6538\n",
            "Epoch 25/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 89.7701 - accuracy: 0.7059 - val_loss: 89.6985 - val_accuracy: 0.6538\n",
            "Epoch 26/500\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 89.6416 - accuracy: 0.7059 - val_loss: 89.5692 - val_accuracy: 0.6538\n",
            "Epoch 27/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 89.5150 - accuracy: 0.7059 - val_loss: 89.4423 - val_accuracy: 0.6538\n",
            "Epoch 28/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 89.3890 - accuracy: 0.7059 - val_loss: 89.3159 - val_accuracy: 0.6538\n",
            "Epoch 29/500\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 89.2628 - accuracy: 0.7059 - val_loss: 89.1909 - val_accuracy: 0.6538\n",
            "Epoch 30/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 89.1365 - accuracy: 0.7059 - val_loss: 89.0644 - val_accuracy: 0.6538\n",
            "Epoch 31/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 89.0104 - accuracy: 0.7059 - val_loss: 88.9402 - val_accuracy: 0.6538\n",
            "Epoch 32/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 88.8875 - accuracy: 0.7059 - val_loss: 88.8174 - val_accuracy: 0.6538\n",
            "Epoch 33/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 88.7621 - accuracy: 0.7059 - val_loss: 88.6905 - val_accuracy: 0.6538\n",
            "Epoch 34/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 88.6373 - accuracy: 0.7059 - val_loss: 88.5644 - val_accuracy: 0.6538\n",
            "Epoch 35/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 88.5132 - accuracy: 0.7059 - val_loss: 88.4411 - val_accuracy: 0.6538\n",
            "Epoch 36/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 88.3882 - accuracy: 0.7059 - val_loss: 88.3188 - val_accuracy: 0.6538\n",
            "Epoch 37/500\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 88.2660 - accuracy: 0.7059 - val_loss: 88.1966 - val_accuracy: 0.6538\n",
            "Epoch 38/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 88.1424 - accuracy: 0.7059 - val_loss: 88.0727 - val_accuracy: 0.6538\n",
            "Epoch 39/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 88.0186 - accuracy: 0.7059 - val_loss: 87.9487 - val_accuracy: 0.6538\n",
            "Epoch 40/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 87.8974 - accuracy: 0.7059 - val_loss: 87.8264 - val_accuracy: 0.6538\n",
            "Epoch 41/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 87.7748 - accuracy: 0.7059 - val_loss: 87.7052 - val_accuracy: 0.6538\n",
            "Epoch 42/500\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 87.6524 - accuracy: 0.7059 - val_loss: 87.5853 - val_accuracy: 0.6538\n",
            "Epoch 43/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 87.5300 - accuracy: 0.7059 - val_loss: 87.4648 - val_accuracy: 0.6538\n",
            "Epoch 44/500\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 87.4099 - accuracy: 0.7059 - val_loss: 87.3429 - val_accuracy: 0.6538\n",
            "Epoch 45/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 87.2875 - accuracy: 0.7059 - val_loss: 87.2218 - val_accuracy: 0.6538\n",
            "Epoch 46/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 87.1670 - accuracy: 0.7059 - val_loss: 87.1017 - val_accuracy: 0.6538\n",
            "Epoch 47/500\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 87.0472 - accuracy: 0.7059 - val_loss: 86.9820 - val_accuracy: 0.6538\n",
            "Epoch 48/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 86.9282 - accuracy: 0.7059 - val_loss: 86.8631 - val_accuracy: 0.6538\n",
            "Epoch 49/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 86.8069 - accuracy: 0.7059 - val_loss: 86.7439 - val_accuracy: 0.6538\n",
            "Epoch 50/500\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 86.6876 - accuracy: 0.7059 - val_loss: 86.6263 - val_accuracy: 0.6538\n",
            "Epoch 51/500\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 86.5679 - accuracy: 0.7059 - val_loss: 86.5083 - val_accuracy: 0.6538\n",
            "Epoch 52/500\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 86.4494 - accuracy: 0.7059 - val_loss: 86.3893 - val_accuracy: 0.6538\n",
            "Epoch 53/500\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 86.3310 - accuracy: 0.7059 - val_loss: 86.2693 - val_accuracy: 0.6538\n",
            "Epoch 54/500\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 86.2119 - accuracy: 0.7059 - val_loss: 86.1497 - val_accuracy: 0.6538\n",
            "Epoch 55/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 86.0959 - accuracy: 0.7059 - val_loss: 86.0303 - val_accuracy: 0.6538\n",
            "Epoch 56/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 85.9760 - accuracy: 0.7059 - val_loss: 85.9141 - val_accuracy: 0.6538\n",
            "Epoch 57/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 85.8601 - accuracy: 0.7059 - val_loss: 85.7982 - val_accuracy: 0.6538\n",
            "Epoch 58/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 85.7416 - accuracy: 0.7059 - val_loss: 85.6788 - val_accuracy: 0.6538\n",
            "Epoch 59/500\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 85.6239 - accuracy: 0.7059 - val_loss: 85.5617 - val_accuracy: 0.6538\n",
            "Epoch 60/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 85.5081 - accuracy: 0.7059 - val_loss: 85.4451 - val_accuracy: 0.6538\n",
            "Epoch 61/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 85.3923 - accuracy: 0.7059 - val_loss: 85.3297 - val_accuracy: 0.6538\n",
            "Epoch 62/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 85.2759 - accuracy: 0.7059 - val_loss: 85.2150 - val_accuracy: 0.6538\n",
            "Epoch 63/500\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 85.1596 - accuracy: 0.7059 - val_loss: 85.0998 - val_accuracy: 0.6538\n",
            "Epoch 64/500\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 85.0440 - accuracy: 0.7059 - val_loss: 84.9853 - val_accuracy: 0.6538\n",
            "Epoch 65/500\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 84.9287 - accuracy: 0.7059 - val_loss: 84.8720 - val_accuracy: 0.6538\n",
            "Epoch 66/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 84.8166 - accuracy: 0.7059 - val_loss: 84.7590 - val_accuracy: 0.6538\n",
            "Epoch 67/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 84.6995 - accuracy: 0.7059 - val_loss: 84.6426 - val_accuracy: 0.6538\n",
            "Epoch 68/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 84.5871 - accuracy: 0.7059 - val_loss: 84.5288 - val_accuracy: 0.6538\n",
            "Epoch 69/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 84.4716 - accuracy: 0.7059 - val_loss: 84.4150 - val_accuracy: 0.6538\n",
            "Epoch 70/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 84.3569 - accuracy: 0.7059 - val_loss: 84.3048 - val_accuracy: 0.6538\n",
            "Epoch 71/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 84.2440 - accuracy: 0.7059 - val_loss: 84.1925 - val_accuracy: 0.6538\n",
            "Epoch 72/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 84.1328 - accuracy: 0.7059 - val_loss: 84.0774 - val_accuracy: 0.6538\n",
            "Epoch 73/500\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 84.0178 - accuracy: 0.7059 - val_loss: 83.9635 - val_accuracy: 0.6538\n",
            "Epoch 74/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 83.9043 - accuracy: 0.7059 - val_loss: 83.8495 - val_accuracy: 0.6538\n",
            "Epoch 75/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 83.7939 - accuracy: 0.7059 - val_loss: 83.7357 - val_accuracy: 0.6538\n",
            "Epoch 76/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 83.6813 - accuracy: 0.7059 - val_loss: 83.6230 - val_accuracy: 0.6538\n",
            "Epoch 77/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 83.5683 - accuracy: 0.7059 - val_loss: 83.5108 - val_accuracy: 0.6538\n",
            "Epoch 78/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 83.4570 - accuracy: 0.7059 - val_loss: 83.4005 - val_accuracy: 0.6538\n",
            "Epoch 79/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 83.3459 - accuracy: 0.7059 - val_loss: 83.2877 - val_accuracy: 0.6538\n",
            "Epoch 80/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 83.2343 - accuracy: 0.7059 - val_loss: 83.1765 - val_accuracy: 0.6538\n",
            "Epoch 81/500\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 83.1244 - accuracy: 0.7059 - val_loss: 83.0655 - val_accuracy: 0.6538\n",
            "Epoch 82/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 83.0158 - accuracy: 0.7059 - val_loss: 82.9539 - val_accuracy: 0.6538\n",
            "Epoch 83/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 82.9037 - accuracy: 0.7059 - val_loss: 82.8445 - val_accuracy: 0.6538\n",
            "Epoch 84/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 82.7934 - accuracy: 0.7059 - val_loss: 82.7346 - val_accuracy: 0.6538\n",
            "Epoch 85/500\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 82.6829 - accuracy: 0.7059 - val_loss: 82.6261 - val_accuracy: 0.6538\n",
            "Epoch 86/500\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 82.5733 - accuracy: 0.7059 - val_loss: 82.5206 - val_accuracy: 0.6538\n",
            "Epoch 87/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 82.4657 - accuracy: 0.7059 - val_loss: 82.4135 - val_accuracy: 0.6538\n",
            "Epoch 88/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 82.3565 - accuracy: 0.7059 - val_loss: 82.3082 - val_accuracy: 0.6538\n",
            "Epoch 89/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 82.2481 - accuracy: 0.7059 - val_loss: 82.2006 - val_accuracy: 0.6538\n",
            "Epoch 90/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 82.1386 - accuracy: 0.7059 - val_loss: 82.0900 - val_accuracy: 0.6538\n",
            "Epoch 91/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 82.0303 - accuracy: 0.7059 - val_loss: 81.9812 - val_accuracy: 0.6538\n",
            "Epoch 92/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 81.9236 - accuracy: 0.7059 - val_loss: 81.8729 - val_accuracy: 0.6538\n",
            "Epoch 93/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 81.8172 - accuracy: 0.7059 - val_loss: 81.7644 - val_accuracy: 0.6538\n",
            "Epoch 94/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 81.7094 - accuracy: 0.7059 - val_loss: 81.6568 - val_accuracy: 0.6538\n",
            "Epoch 95/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 81.5999 - accuracy: 0.7059 - val_loss: 81.5551 - val_accuracy: 0.6538\n",
            "Epoch 96/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 81.4956 - accuracy: 0.7059 - val_loss: 81.4505 - val_accuracy: 0.6538\n",
            "Epoch 97/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 81.3898 - accuracy: 0.7059 - val_loss: 81.3401 - val_accuracy: 0.6538\n",
            "Epoch 98/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 81.2835 - accuracy: 0.7059 - val_loss: 81.2317 - val_accuracy: 0.6538\n",
            "Epoch 99/500\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 81.1756 - accuracy: 0.7059 - val_loss: 81.1256 - val_accuracy: 0.6538\n",
            "Epoch 100/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 81.0715 - accuracy: 0.7059 - val_loss: 81.0215 - val_accuracy: 0.6538\n",
            "Epoch 101/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 80.9650 - accuracy: 0.7059 - val_loss: 80.9193 - val_accuracy: 0.6538\n",
            "Epoch 102/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 80.8591 - accuracy: 0.7059 - val_loss: 80.8169 - val_accuracy: 0.6538\n",
            "Epoch 103/500\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 80.7542 - accuracy: 0.7059 - val_loss: 80.7129 - val_accuracy: 0.6538\n",
            "Epoch 104/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 80.6492 - accuracy: 0.7059 - val_loss: 80.6063 - val_accuracy: 0.6538\n",
            "Epoch 105/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 80.5444 - accuracy: 0.7059 - val_loss: 80.5015 - val_accuracy: 0.6538\n",
            "Epoch 106/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 80.4400 - accuracy: 0.7059 - val_loss: 80.3974 - val_accuracy: 0.6538\n",
            "Epoch 107/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 80.3363 - accuracy: 0.7059 - val_loss: 80.2928 - val_accuracy: 0.6538\n",
            "Epoch 108/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 80.2330 - accuracy: 0.7059 - val_loss: 80.1900 - val_accuracy: 0.6538\n",
            "Epoch 109/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 80.1290 - accuracy: 0.7059 - val_loss: 80.0892 - val_accuracy: 0.6538\n",
            "Epoch 110/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 80.0261 - accuracy: 0.7059 - val_loss: 79.9858 - val_accuracy: 0.6538\n",
            "Epoch 111/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 79.9238 - accuracy: 0.7059 - val_loss: 79.8818 - val_accuracy: 0.6538\n",
            "Epoch 112/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 79.8196 - accuracy: 0.7059 - val_loss: 79.7792 - val_accuracy: 0.6538\n",
            "Epoch 113/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 79.7176 - accuracy: 0.7059 - val_loss: 79.6760 - val_accuracy: 0.6538\n",
            "Epoch 114/500\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 79.6149 - accuracy: 0.7059 - val_loss: 79.5717 - val_accuracy: 0.6538\n",
            "Epoch 115/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 79.5135 - accuracy: 0.7059 - val_loss: 79.4685 - val_accuracy: 0.6538\n",
            "Epoch 116/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 79.4113 - accuracy: 0.7059 - val_loss: 79.3670 - val_accuracy: 0.6538\n",
            "Epoch 117/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 79.3087 - accuracy: 0.7059 - val_loss: 79.2660 - val_accuracy: 0.6538\n",
            "Epoch 118/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 79.2079 - accuracy: 0.7059 - val_loss: 79.1649 - val_accuracy: 0.6538\n",
            "Epoch 119/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 79.1067 - accuracy: 0.7059 - val_loss: 79.0631 - val_accuracy: 0.6538\n",
            "Epoch 120/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 79.0051 - accuracy: 0.7059 - val_loss: 78.9626 - val_accuracy: 0.6538\n",
            "Epoch 121/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 78.9046 - accuracy: 0.7059 - val_loss: 78.8630 - val_accuracy: 0.6538\n",
            "Epoch 122/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 78.8046 - accuracy: 0.7059 - val_loss: 78.7628 - val_accuracy: 0.6538\n",
            "Epoch 123/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 78.7033 - accuracy: 0.7059 - val_loss: 78.6630 - val_accuracy: 0.6538\n",
            "Epoch 124/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 78.6047 - accuracy: 0.7059 - val_loss: 78.5640 - val_accuracy: 0.6538\n",
            "Epoch 125/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 78.5039 - accuracy: 0.7059 - val_loss: 78.4661 - val_accuracy: 0.6538\n",
            "Epoch 126/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 78.4069 - accuracy: 0.7059 - val_loss: 78.3690 - val_accuracy: 0.6538\n",
            "Epoch 127/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 78.3074 - accuracy: 0.7059 - val_loss: 78.2702 - val_accuracy: 0.6538\n",
            "Epoch 128/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 78.2083 - accuracy: 0.7059 - val_loss: 78.1746 - val_accuracy: 0.6538\n",
            "Epoch 129/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 78.1089 - accuracy: 0.7059 - val_loss: 78.0775 - val_accuracy: 0.6538\n",
            "Epoch 130/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 78.0119 - accuracy: 0.7059 - val_loss: 77.9774 - val_accuracy: 0.6538\n",
            "Epoch 131/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 77.9121 - accuracy: 0.7059 - val_loss: 77.8771 - val_accuracy: 0.6538\n",
            "Epoch 132/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 77.8134 - accuracy: 0.7059 - val_loss: 77.7754 - val_accuracy: 0.6538\n",
            "Epoch 133/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 77.7150 - accuracy: 0.7059 - val_loss: 77.6758 - val_accuracy: 0.6538\n",
            "Epoch 134/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 77.6171 - accuracy: 0.7059 - val_loss: 77.5794 - val_accuracy: 0.6538\n",
            "Epoch 135/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 77.5198 - accuracy: 0.7059 - val_loss: 77.4848 - val_accuracy: 0.6538\n",
            "Epoch 136/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 77.4244 - accuracy: 0.7059 - val_loss: 77.3895 - val_accuracy: 0.6538\n",
            "Epoch 137/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 77.3262 - accuracy: 0.7059 - val_loss: 77.2919 - val_accuracy: 0.6538\n",
            "Epoch 138/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 77.2290 - accuracy: 0.7059 - val_loss: 77.1958 - val_accuracy: 0.6538\n",
            "Epoch 139/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 77.1332 - accuracy: 0.7059 - val_loss: 77.0994 - val_accuracy: 0.6538\n",
            "Epoch 140/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 77.0364 - accuracy: 0.7059 - val_loss: 77.0020 - val_accuracy: 0.6538\n",
            "Epoch 141/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 76.9410 - accuracy: 0.7059 - val_loss: 76.9054 - val_accuracy: 0.6538\n",
            "Epoch 142/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 76.8435 - accuracy: 0.7059 - val_loss: 76.8071 - val_accuracy: 0.6538\n",
            "Epoch 143/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 76.7486 - accuracy: 0.7059 - val_loss: 76.7105 - val_accuracy: 0.6538\n",
            "Epoch 144/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 76.6535 - accuracy: 0.7059 - val_loss: 76.6151 - val_accuracy: 0.6538\n",
            "Epoch 145/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 76.5580 - accuracy: 0.7059 - val_loss: 76.5213 - val_accuracy: 0.6538\n",
            "Epoch 146/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 76.4652 - accuracy: 0.7059 - val_loss: 76.4258 - val_accuracy: 0.6538\n",
            "Epoch 147/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 76.3674 - accuracy: 0.7059 - val_loss: 76.3319 - val_accuracy: 0.6538\n",
            "Epoch 148/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 76.2749 - accuracy: 0.7059 - val_loss: 76.2391 - val_accuracy: 0.6538\n",
            "Epoch 149/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 76.1799 - accuracy: 0.7059 - val_loss: 76.1435 - val_accuracy: 0.6538\n",
            "Epoch 150/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 76.0847 - accuracy: 0.7059 - val_loss: 76.0492 - val_accuracy: 0.6538\n",
            "Epoch 151/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 75.9907 - accuracy: 0.7059 - val_loss: 75.9571 - val_accuracy: 0.6538\n",
            "Epoch 152/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 75.8976 - accuracy: 0.7059 - val_loss: 75.8658 - val_accuracy: 0.6538\n",
            "Epoch 153/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 75.8041 - accuracy: 0.7059 - val_loss: 75.7743 - val_accuracy: 0.6538\n",
            "Epoch 154/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 75.7111 - accuracy: 0.7059 - val_loss: 75.6852 - val_accuracy: 0.6538\n",
            "Epoch 155/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 75.6189 - accuracy: 0.7059 - val_loss: 75.5920 - val_accuracy: 0.6538\n",
            "Epoch 156/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 75.5261 - accuracy: 0.7059 - val_loss: 75.4955 - val_accuracy: 0.6538\n",
            "Epoch 157/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 75.4326 - accuracy: 0.7059 - val_loss: 75.4024 - val_accuracy: 0.6538\n",
            "Epoch 158/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 75.3392 - accuracy: 0.7059 - val_loss: 75.3100 - val_accuracy: 0.6538\n",
            "Epoch 159/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 75.2468 - accuracy: 0.7059 - val_loss: 75.2179 - val_accuracy: 0.6538\n",
            "Epoch 160/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 75.1548 - accuracy: 0.7059 - val_loss: 75.1255 - val_accuracy: 0.6538\n",
            "Epoch 161/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 75.0626 - accuracy: 0.7059 - val_loss: 75.0332 - val_accuracy: 0.6538\n",
            "Epoch 162/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 74.9718 - accuracy: 0.7059 - val_loss: 74.9398 - val_accuracy: 0.6538\n",
            "Epoch 163/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 74.8794 - accuracy: 0.7059 - val_loss: 74.8465 - val_accuracy: 0.6538\n",
            "Epoch 164/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 74.7897 - accuracy: 0.7059 - val_loss: 74.7557 - val_accuracy: 0.6538\n",
            "Epoch 165/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 74.6997 - accuracy: 0.7059 - val_loss: 74.6653 - val_accuracy: 0.6538\n",
            "Epoch 166/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 74.6073 - accuracy: 0.7059 - val_loss: 74.5746 - val_accuracy: 0.6538\n",
            "Epoch 167/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 74.5189 - accuracy: 0.7059 - val_loss: 74.4856 - val_accuracy: 0.6538\n",
            "Epoch 168/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 74.4257 - accuracy: 0.7059 - val_loss: 74.3953 - val_accuracy: 0.6538\n",
            "Epoch 169/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 74.3349 - accuracy: 0.7059 - val_loss: 74.3061 - val_accuracy: 0.6538\n",
            "Epoch 170/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 74.2450 - accuracy: 0.7059 - val_loss: 74.2178 - val_accuracy: 0.6538\n",
            "Epoch 171/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 74.1559 - accuracy: 0.7059 - val_loss: 74.1297 - val_accuracy: 0.6538\n",
            "Epoch 172/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 74.0656 - accuracy: 0.7059 - val_loss: 74.0395 - val_accuracy: 0.6538\n",
            "Epoch 173/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 73.9757 - accuracy: 0.7059 - val_loss: 73.9508 - val_accuracy: 0.6538\n",
            "Epoch 174/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 73.8886 - accuracy: 0.7059 - val_loss: 73.8628 - val_accuracy: 0.6538\n",
            "Epoch 175/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 73.7986 - accuracy: 0.7059 - val_loss: 73.7747 - val_accuracy: 0.6538\n",
            "Epoch 176/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 73.7086 - accuracy: 0.7059 - val_loss: 73.6856 - val_accuracy: 0.6538\n",
            "Epoch 177/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 73.6207 - accuracy: 0.7059 - val_loss: 73.5977 - val_accuracy: 0.6538\n",
            "Epoch 178/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 73.5318 - accuracy: 0.7059 - val_loss: 73.5074 - val_accuracy: 0.6538\n",
            "Epoch 179/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 73.4435 - accuracy: 0.7059 - val_loss: 73.4178 - val_accuracy: 0.6538\n",
            "Epoch 180/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 73.3563 - accuracy: 0.7059 - val_loss: 73.3310 - val_accuracy: 0.6538\n",
            "Epoch 181/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 73.2691 - accuracy: 0.7059 - val_loss: 73.2433 - val_accuracy: 0.6538\n",
            "Epoch 182/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 73.1796 - accuracy: 0.7059 - val_loss: 73.1575 - val_accuracy: 0.6538\n",
            "Epoch 183/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 73.0908 - accuracy: 0.7059 - val_loss: 73.0742 - val_accuracy: 0.6538\n",
            "Epoch 184/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 73.0059 - accuracy: 0.7059 - val_loss: 72.9911 - val_accuracy: 0.6538\n",
            "Epoch 185/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 72.9195 - accuracy: 0.7059 - val_loss: 72.9021 - val_accuracy: 0.6538\n",
            "Epoch 186/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 72.8307 - accuracy: 0.7059 - val_loss: 72.8141 - val_accuracy: 0.6538\n",
            "Epoch 187/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 72.7431 - accuracy: 0.7059 - val_loss: 72.7260 - val_accuracy: 0.6538\n",
            "Epoch 188/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 72.6602 - accuracy: 0.7059 - val_loss: 72.6393 - val_accuracy: 0.6538\n",
            "Epoch 189/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 72.5769 - accuracy: 0.7059 - val_loss: 72.5554 - val_accuracy: 0.6538\n",
            "Epoch 190/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 72.4857 - accuracy: 0.7059 - val_loss: 72.4684 - val_accuracy: 0.6538\n",
            "Epoch 191/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 72.3986 - accuracy: 0.7059 - val_loss: 72.3816 - val_accuracy: 0.6538\n",
            "Epoch 192/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 72.3127 - accuracy: 0.7059 - val_loss: 72.2961 - val_accuracy: 0.6538\n",
            "Epoch 193/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 72.2267 - accuracy: 0.7059 - val_loss: 72.2121 - val_accuracy: 0.6538\n",
            "Epoch 194/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 72.1425 - accuracy: 0.7059 - val_loss: 72.1285 - val_accuracy: 0.6538\n",
            "Epoch 195/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 72.0565 - accuracy: 0.7059 - val_loss: 72.0442 - val_accuracy: 0.6538\n",
            "Epoch 196/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 71.9722 - accuracy: 0.7059 - val_loss: 71.9599 - val_accuracy: 0.6538\n",
            "Epoch 197/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 71.8863 - accuracy: 0.7059 - val_loss: 71.8771 - val_accuracy: 0.6538\n",
            "Epoch 198/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 71.8021 - accuracy: 0.7059 - val_loss: 71.7916 - val_accuracy: 0.6538\n",
            "Epoch 199/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 71.7171 - accuracy: 0.7059 - val_loss: 71.7065 - val_accuracy: 0.6538\n",
            "Epoch 200/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 71.6326 - accuracy: 0.7059 - val_loss: 71.6229 - val_accuracy: 0.6538\n",
            "Epoch 201/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 71.5488 - accuracy: 0.7059 - val_loss: 71.5404 - val_accuracy: 0.6538\n",
            "Epoch 202/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 71.4654 - accuracy: 0.7059 - val_loss: 71.4589 - val_accuracy: 0.6538\n",
            "Epoch 203/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 71.3811 - accuracy: 0.7059 - val_loss: 71.3748 - val_accuracy: 0.6538\n",
            "Epoch 204/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 71.2976 - accuracy: 0.7059 - val_loss: 71.2916 - val_accuracy: 0.6538\n",
            "Epoch 205/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 71.2136 - accuracy: 0.7059 - val_loss: 71.2094 - val_accuracy: 0.6538\n",
            "Epoch 206/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 71.1322 - accuracy: 0.7059 - val_loss: 71.1256 - val_accuracy: 0.6538\n",
            "Epoch 207/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 71.0501 - accuracy: 0.7059 - val_loss: 71.0455 - val_accuracy: 0.6538\n",
            "Epoch 208/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 70.9670 - accuracy: 0.7059 - val_loss: 70.9609 - val_accuracy: 0.6538\n",
            "Epoch 209/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 70.8811 - accuracy: 0.7059 - val_loss: 70.8807 - val_accuracy: 0.6538\n",
            "Epoch 210/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 70.8007 - accuracy: 0.7059 - val_loss: 70.7990 - val_accuracy: 0.6538\n",
            "Epoch 211/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 70.7182 - accuracy: 0.7059 - val_loss: 70.7137 - val_accuracy: 0.6538\n",
            "Epoch 212/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 70.6351 - accuracy: 0.7059 - val_loss: 70.6319 - val_accuracy: 0.6538\n",
            "Epoch 213/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 70.5540 - accuracy: 0.7059 - val_loss: 70.5513 - val_accuracy: 0.6538\n",
            "Epoch 214/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 70.4694 - accuracy: 0.7059 - val_loss: 70.4724 - val_accuracy: 0.6538\n",
            "Epoch 215/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 70.3904 - accuracy: 0.7059 - val_loss: 70.3930 - val_accuracy: 0.6538\n",
            "Epoch 216/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 70.3100 - accuracy: 0.7059 - val_loss: 70.3087 - val_accuracy: 0.6538\n",
            "Epoch 217/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 70.2264 - accuracy: 0.7059 - val_loss: 70.2289 - val_accuracy: 0.6538\n",
            "Epoch 218/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 70.1442 - accuracy: 0.7059 - val_loss: 70.1479 - val_accuracy: 0.6538\n",
            "Epoch 219/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 70.0649 - accuracy: 0.7059 - val_loss: 70.0671 - val_accuracy: 0.6538\n",
            "Epoch 220/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 69.9833 - accuracy: 0.7059 - val_loss: 69.9867 - val_accuracy: 0.6538\n",
            "Epoch 221/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 69.9024 - accuracy: 0.7059 - val_loss: 69.9080 - val_accuracy: 0.6538\n",
            "Epoch 222/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 69.8213 - accuracy: 0.7059 - val_loss: 69.8298 - val_accuracy: 0.6538\n",
            "Epoch 223/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 69.7434 - accuracy: 0.7059 - val_loss: 69.7499 - val_accuracy: 0.6538\n",
            "Epoch 224/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 69.6638 - accuracy: 0.7059 - val_loss: 69.6657 - val_accuracy: 0.6538\n",
            "Epoch 225/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 69.5837 - accuracy: 0.7059 - val_loss: 69.5867 - val_accuracy: 0.6538\n",
            "Epoch 226/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 69.5027 - accuracy: 0.7059 - val_loss: 69.5084 - val_accuracy: 0.6538\n",
            "Epoch 227/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 69.4251 - accuracy: 0.7059 - val_loss: 69.4288 - val_accuracy: 0.6538\n",
            "Epoch 228/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 69.3428 - accuracy: 0.7059 - val_loss: 69.3541 - val_accuracy: 0.6538\n",
            "Epoch 229/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 69.2658 - accuracy: 0.7059 - val_loss: 69.2736 - val_accuracy: 0.6538\n",
            "Epoch 230/500\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 69.1833 - accuracy: 0.7059 - val_loss: 69.1933 - val_accuracy: 0.6538\n",
            "Epoch 231/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 69.1076 - accuracy: 0.7118 - val_loss: 69.1156 - val_accuracy: 0.6538\n",
            "Epoch 232/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 69.0294 - accuracy: 0.7118 - val_loss: 69.0375 - val_accuracy: 0.6538\n",
            "Epoch 233/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 68.9483 - accuracy: 0.7059 - val_loss: 68.9662 - val_accuracy: 0.6538\n",
            "Epoch 234/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 68.8802 - accuracy: 0.7059 - val_loss: 68.8883 - val_accuracy: 0.6538\n",
            "Epoch 235/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 68.7949 - accuracy: 0.7059 - val_loss: 68.8029 - val_accuracy: 0.6538\n",
            "Epoch 236/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 68.7236 - accuracy: 0.7059 - val_loss: 68.7251 - val_accuracy: 0.6538\n",
            "Epoch 237/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 68.6366 - accuracy: 0.7059 - val_loss: 68.6534 - val_accuracy: 0.6538\n",
            "Epoch 238/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 68.5676 - accuracy: 0.7059 - val_loss: 68.5901 - val_accuracy: 0.6538\n",
            "Epoch 239/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 68.4868 - accuracy: 0.7059 - val_loss: 68.4948 - val_accuracy: 0.6538\n",
            "Epoch 240/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 68.4061 - accuracy: 0.7118 - val_loss: 68.4188 - val_accuracy: 0.6538\n",
            "Epoch 241/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 68.3420 - accuracy: 0.7059 - val_loss: 68.3410 - val_accuracy: 0.6538\n",
            "Epoch 242/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 68.2532 - accuracy: 0.7118 - val_loss: 68.2812 - val_accuracy: 0.6538\n",
            "Epoch 243/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 68.1896 - accuracy: 0.7059 - val_loss: 68.1996 - val_accuracy: 0.6538\n",
            "Epoch 244/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 68.1036 - accuracy: 0.7059 - val_loss: 68.1100 - val_accuracy: 0.6538\n",
            "Epoch 245/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 68.0295 - accuracy: 0.7118 - val_loss: 68.0339 - val_accuracy: 0.6538\n",
            "Epoch 246/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 67.9480 - accuracy: 0.7059 - val_loss: 67.9601 - val_accuracy: 0.6538\n",
            "Epoch 247/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 67.8724 - accuracy: 0.7059 - val_loss: 67.8982 - val_accuracy: 0.6538\n",
            "Epoch 248/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 67.8030 - accuracy: 0.7059 - val_loss: 67.8120 - val_accuracy: 0.6538\n",
            "Epoch 249/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 67.7206 - accuracy: 0.7118 - val_loss: 67.7322 - val_accuracy: 0.6538\n",
            "Epoch 250/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 67.6457 - accuracy: 0.7118 - val_loss: 67.6581 - val_accuracy: 0.6538\n",
            "Epoch 251/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 67.5723 - accuracy: 0.7118 - val_loss: 67.5859 - val_accuracy: 0.6538\n",
            "Epoch 252/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 67.4949 - accuracy: 0.7118 - val_loss: 67.5144 - val_accuracy: 0.6538\n",
            "Epoch 253/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 67.4189 - accuracy: 0.7059 - val_loss: 67.4378 - val_accuracy: 0.6538\n",
            "Epoch 254/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 67.3434 - accuracy: 0.7059 - val_loss: 67.3640 - val_accuracy: 0.6538\n",
            "Epoch 255/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 67.2692 - accuracy: 0.7118 - val_loss: 67.2918 - val_accuracy: 0.6538\n",
            "Epoch 256/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 67.1964 - accuracy: 0.7118 - val_loss: 67.2200 - val_accuracy: 0.6538\n",
            "Epoch 257/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 67.1195 - accuracy: 0.7118 - val_loss: 67.1448 - val_accuracy: 0.6538\n",
            "Epoch 258/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 67.0468 - accuracy: 0.7118 - val_loss: 67.0708 - val_accuracy: 0.6538\n",
            "Epoch 259/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 66.9751 - accuracy: 0.7118 - val_loss: 66.9995 - val_accuracy: 0.6538\n",
            "Epoch 260/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 66.8992 - accuracy: 0.7118 - val_loss: 66.9300 - val_accuracy: 0.6538\n",
            "Epoch 261/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 66.8292 - accuracy: 0.7059 - val_loss: 66.8505 - val_accuracy: 0.6538\n",
            "Epoch 262/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 66.7616 - accuracy: 0.7118 - val_loss: 66.7773 - val_accuracy: 0.6538\n",
            "Epoch 263/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 66.6855 - accuracy: 0.7118 - val_loss: 66.7047 - val_accuracy: 0.6538\n",
            "Epoch 264/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 66.6078 - accuracy: 0.7118 - val_loss: 66.6396 - val_accuracy: 0.6538\n",
            "Epoch 265/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 66.5341 - accuracy: 0.7059 - val_loss: 66.5580 - val_accuracy: 0.6538\n",
            "Epoch 266/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 66.4627 - accuracy: 0.7118 - val_loss: 66.4843 - val_accuracy: 0.6538\n",
            "Epoch 267/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 66.3957 - accuracy: 0.7118 - val_loss: 66.4103 - val_accuracy: 0.6538\n",
            "Epoch 268/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 66.3124 - accuracy: 0.7118 - val_loss: 66.3495 - val_accuracy: 0.6538\n",
            "Epoch 269/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 66.2478 - accuracy: 0.7118 - val_loss: 66.2764 - val_accuracy: 0.6538\n",
            "Epoch 270/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 66.1751 - accuracy: 0.7118 - val_loss: 66.1922 - val_accuracy: 0.6538\n",
            "Epoch 271/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 66.1009 - accuracy: 0.7118 - val_loss: 66.1205 - val_accuracy: 0.6538\n",
            "Epoch 272/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 66.0263 - accuracy: 0.7118 - val_loss: 66.0521 - val_accuracy: 0.6538\n",
            "Epoch 273/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 65.9541 - accuracy: 0.7118 - val_loss: 65.9880 - val_accuracy: 0.6538\n",
            "Epoch 274/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 65.8848 - accuracy: 0.7118 - val_loss: 65.9138 - val_accuracy: 0.6538\n",
            "Epoch 275/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 65.8102 - accuracy: 0.7118 - val_loss: 65.8389 - val_accuracy: 0.6538\n",
            "Epoch 276/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 65.7452 - accuracy: 0.7118 - val_loss: 65.7681 - val_accuracy: 0.6538\n",
            "Epoch 277/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 65.6722 - accuracy: 0.7118 - val_loss: 65.7040 - val_accuracy: 0.6538\n",
            "Epoch 278/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 65.5985 - accuracy: 0.7118 - val_loss: 65.6349 - val_accuracy: 0.6538\n",
            "Epoch 279/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 65.5258 - accuracy: 0.7118 - val_loss: 65.5595 - val_accuracy: 0.6538\n",
            "Epoch 280/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 65.4536 - accuracy: 0.7118 - val_loss: 65.4878 - val_accuracy: 0.6538\n",
            "Epoch 281/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 65.3885 - accuracy: 0.7118 - val_loss: 65.4186 - val_accuracy: 0.6538\n",
            "Epoch 282/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 65.3139 - accuracy: 0.7118 - val_loss: 65.3520 - val_accuracy: 0.6538\n",
            "Epoch 283/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 65.2433 - accuracy: 0.7118 - val_loss: 65.2804 - val_accuracy: 0.6538\n",
            "Epoch 284/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 65.1755 - accuracy: 0.7118 - val_loss: 65.2085 - val_accuracy: 0.6538\n",
            "Epoch 285/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 65.1022 - accuracy: 0.7118 - val_loss: 65.1432 - val_accuracy: 0.6538\n",
            "Epoch 286/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 65.0336 - accuracy: 0.7118 - val_loss: 65.0794 - val_accuracy: 0.6538\n",
            "Epoch 287/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 64.9689 - accuracy: 0.7118 - val_loss: 65.0003 - val_accuracy: 0.6538\n",
            "Epoch 288/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 64.8935 - accuracy: 0.7118 - val_loss: 64.9334 - val_accuracy: 0.6538\n",
            "Epoch 289/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 64.8233 - accuracy: 0.7118 - val_loss: 64.8670 - val_accuracy: 0.6538\n",
            "Epoch 290/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 64.7546 - accuracy: 0.7176 - val_loss: 64.7950 - val_accuracy: 0.6538\n",
            "Epoch 291/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 64.6858 - accuracy: 0.7176 - val_loss: 64.7230 - val_accuracy: 0.6538\n",
            "Epoch 292/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 64.6155 - accuracy: 0.7176 - val_loss: 64.6609 - val_accuracy: 0.6538\n",
            "Epoch 293/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 64.5517 - accuracy: 0.7176 - val_loss: 64.5923 - val_accuracy: 0.6538\n",
            "Epoch 294/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 64.4761 - accuracy: 0.7176 - val_loss: 64.5161 - val_accuracy: 0.6538\n",
            "Epoch 295/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 64.4124 - accuracy: 0.7176 - val_loss: 64.4494 - val_accuracy: 0.6538\n",
            "Epoch 296/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 64.3430 - accuracy: 0.7176 - val_loss: 64.3918 - val_accuracy: 0.6538\n",
            "Epoch 297/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 64.2711 - accuracy: 0.7176 - val_loss: 64.3159 - val_accuracy: 0.6538\n",
            "Epoch 298/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 64.2056 - accuracy: 0.7176 - val_loss: 64.2466 - val_accuracy: 0.6538\n",
            "Epoch 299/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 64.1357 - accuracy: 0.7176 - val_loss: 64.1863 - val_accuracy: 0.6538\n",
            "Epoch 300/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 64.0657 - accuracy: 0.7176 - val_loss: 64.1182 - val_accuracy: 0.6538\n",
            "Epoch 301/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 63.9963 - accuracy: 0.7176 - val_loss: 64.0428 - val_accuracy: 0.6538\n",
            "Epoch 302/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 63.9318 - accuracy: 0.7176 - val_loss: 63.9778 - val_accuracy: 0.6538\n",
            "Epoch 303/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 63.8606 - accuracy: 0.7176 - val_loss: 63.9107 - val_accuracy: 0.6538\n",
            "Epoch 304/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 63.7906 - accuracy: 0.7176 - val_loss: 63.8488 - val_accuracy: 0.6538\n",
            "Epoch 305/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 63.7276 - accuracy: 0.7176 - val_loss: 63.7864 - val_accuracy: 0.6538\n",
            "Epoch 306/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 63.6600 - accuracy: 0.7176 - val_loss: 63.7068 - val_accuracy: 0.6538\n",
            "Epoch 307/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 63.5907 - accuracy: 0.7176 - val_loss: 63.6398 - val_accuracy: 0.6538\n",
            "Epoch 308/500\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 63.5203 - accuracy: 0.7176 - val_loss: 63.5776 - val_accuracy: 0.6538\n",
            "Epoch 309/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 63.4546 - accuracy: 0.7176 - val_loss: 63.5136 - val_accuracy: 0.6538\n",
            "Epoch 310/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 63.3869 - accuracy: 0.7176 - val_loss: 63.4409 - val_accuracy: 0.6538\n",
            "Epoch 311/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 63.3241 - accuracy: 0.7176 - val_loss: 63.3750 - val_accuracy: 0.6538\n",
            "Epoch 312/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 63.2580 - accuracy: 0.7176 - val_loss: 63.3197 - val_accuracy: 0.6538\n",
            "Epoch 313/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 63.1977 - accuracy: 0.7176 - val_loss: 63.2490 - val_accuracy: 0.6538\n",
            "Epoch 314/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 63.1210 - accuracy: 0.7176 - val_loss: 63.1759 - val_accuracy: 0.6538\n",
            "Epoch 315/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 63.0631 - accuracy: 0.7176 - val_loss: 63.1158 - val_accuracy: 0.6538\n",
            "Epoch 316/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 62.9935 - accuracy: 0.7176 - val_loss: 63.0533 - val_accuracy: 0.6538\n",
            "Epoch 317/500\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 62.9315 - accuracy: 0.7176 - val_loss: 62.9916 - val_accuracy: 0.6538\n",
            "Epoch 318/500\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 62.8602 - accuracy: 0.7176 - val_loss: 62.9174 - val_accuracy: 0.6538\n",
            "Epoch 319/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 62.7959 - accuracy: 0.7176 - val_loss: 62.8544 - val_accuracy: 0.6538\n",
            "Epoch 320/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 62.7376 - accuracy: 0.7176 - val_loss: 62.7986 - val_accuracy: 0.6538\n",
            "Epoch 321/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 62.6630 - accuracy: 0.7176 - val_loss: 62.7193 - val_accuracy: 0.6538\n",
            "Epoch 322/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 62.6030 - accuracy: 0.7176 - val_loss: 62.6544 - val_accuracy: 0.6538\n",
            "Epoch 323/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 62.5344 - accuracy: 0.7176 - val_loss: 62.5969 - val_accuracy: 0.6538\n",
            "Epoch 324/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 62.4746 - accuracy: 0.7176 - val_loss: 62.5359 - val_accuracy: 0.6538\n",
            "Epoch 325/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 62.4061 - accuracy: 0.7176 - val_loss: 62.4600 - val_accuracy: 0.6538\n",
            "Epoch 326/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 62.3435 - accuracy: 0.7176 - val_loss: 62.3983 - val_accuracy: 0.6538\n",
            "Epoch 327/500\n",
            "3/3 [==============================] - 0s 168ms/step - loss: 62.2826 - accuracy: 0.7176 - val_loss: 62.3495 - val_accuracy: 0.6538\n",
            "Epoch 328/500\n",
            "3/3 [==============================] - 0s 102ms/step - loss: 62.2170 - accuracy: 0.7176 - val_loss: 62.2659 - val_accuracy: 0.6538\n",
            "Epoch 329/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 62.1503 - accuracy: 0.7176 - val_loss: 62.2024 - val_accuracy: 0.6538\n",
            "Epoch 330/500\n",
            "3/3 [==============================] - 0s 126ms/step - loss: 62.0903 - accuracy: 0.7176 - val_loss: 62.1491 - val_accuracy: 0.6538\n",
            "Epoch 331/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 62.0214 - accuracy: 0.7176 - val_loss: 62.0752 - val_accuracy: 0.6538\n",
            "Epoch 332/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 61.9565 - accuracy: 0.7176 - val_loss: 62.0117 - val_accuracy: 0.6538\n",
            "Epoch 333/500\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 61.8922 - accuracy: 0.7176 - val_loss: 61.9497 - val_accuracy: 0.6538\n",
            "Epoch 334/500\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 61.8284 - accuracy: 0.7176 - val_loss: 61.8889 - val_accuracy: 0.6538\n",
            "Epoch 335/500\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 61.7647 - accuracy: 0.7176 - val_loss: 61.8279 - val_accuracy: 0.6538\n",
            "Epoch 336/500\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 61.7027 - accuracy: 0.7176 - val_loss: 61.7672 - val_accuracy: 0.6538\n",
            "Epoch 337/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 61.6409 - accuracy: 0.7176 - val_loss: 61.7007 - val_accuracy: 0.6538\n",
            "Epoch 338/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 61.5777 - accuracy: 0.7176 - val_loss: 61.6441 - val_accuracy: 0.6538\n",
            "Epoch 339/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 61.5214 - accuracy: 0.7176 - val_loss: 61.5745 - val_accuracy: 0.6538\n",
            "Epoch 340/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 61.4509 - accuracy: 0.7176 - val_loss: 61.5221 - val_accuracy: 0.6538\n",
            "Epoch 341/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 61.3890 - accuracy: 0.7176 - val_loss: 61.4525 - val_accuracy: 0.6538\n",
            "Epoch 342/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 61.3236 - accuracy: 0.7176 - val_loss: 61.3871 - val_accuracy: 0.6538\n",
            "Epoch 343/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 61.2640 - accuracy: 0.7176 - val_loss: 61.3274 - val_accuracy: 0.6538\n",
            "Epoch 344/500\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 61.2002 - accuracy: 0.7176 - val_loss: 61.2674 - val_accuracy: 0.6538\n",
            "Epoch 345/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 61.1462 - accuracy: 0.7176 - val_loss: 61.2047 - val_accuracy: 0.6538\n",
            "Epoch 346/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 61.0772 - accuracy: 0.7176 - val_loss: 61.1418 - val_accuracy: 0.6538\n",
            "Epoch 347/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 61.0304 - accuracy: 0.7176 - val_loss: 61.0761 - val_accuracy: 0.6538\n",
            "Epoch 348/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 60.9531 - accuracy: 0.7176 - val_loss: 61.0423 - val_accuracy: 0.6538\n",
            "Epoch 349/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 60.8981 - accuracy: 0.7176 - val_loss: 60.9512 - val_accuracy: 0.6538\n",
            "Epoch 350/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 60.8319 - accuracy: 0.7176 - val_loss: 60.8876 - val_accuracy: 0.6538\n",
            "Epoch 351/500\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 60.7745 - accuracy: 0.7176 - val_loss: 60.8340 - val_accuracy: 0.6538\n",
            "Epoch 352/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 60.7076 - accuracy: 0.7176 - val_loss: 60.7717 - val_accuracy: 0.6538\n",
            "Epoch 353/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 60.6589 - accuracy: 0.7176 - val_loss: 60.7068 - val_accuracy: 0.6538\n",
            "Epoch 354/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 60.6043 - accuracy: 0.7176 - val_loss: 60.6556 - val_accuracy: 0.6538\n",
            "Epoch 355/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 60.5234 - accuracy: 0.7176 - val_loss: 60.5821 - val_accuracy: 0.6538\n",
            "Epoch 356/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 60.4733 - accuracy: 0.7176 - val_loss: 60.5214 - val_accuracy: 0.6538\n",
            "Epoch 357/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 60.3999 - accuracy: 0.7176 - val_loss: 60.4850 - val_accuracy: 0.6538\n",
            "Epoch 358/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 60.3491 - accuracy: 0.7176 - val_loss: 60.4023 - val_accuracy: 0.6538\n",
            "Epoch 359/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 60.2805 - accuracy: 0.7176 - val_loss: 60.3429 - val_accuracy: 0.6538\n",
            "Epoch 360/500\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 60.2239 - accuracy: 0.7176 - val_loss: 60.2895 - val_accuracy: 0.6538\n",
            "Epoch 361/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 60.1596 - accuracy: 0.7176 - val_loss: 60.2307 - val_accuracy: 0.6538\n",
            "Epoch 362/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 60.1003 - accuracy: 0.7176 - val_loss: 60.1682 - val_accuracy: 0.6538\n",
            "Epoch 363/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 60.0385 - accuracy: 0.7176 - val_loss: 60.1084 - val_accuracy: 0.6538\n",
            "Epoch 364/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 59.9778 - accuracy: 0.7176 - val_loss: 60.0540 - val_accuracy: 0.6538\n",
            "Epoch 365/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 59.9243 - accuracy: 0.7176 - val_loss: 59.9889 - val_accuracy: 0.6538\n",
            "Epoch 366/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 59.8591 - accuracy: 0.7176 - val_loss: 59.9299 - val_accuracy: 0.6538\n",
            "Epoch 367/500\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 59.8024 - accuracy: 0.7176 - val_loss: 59.8812 - val_accuracy: 0.6538\n",
            "Epoch 368/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 59.7425 - accuracy: 0.7176 - val_loss: 59.8090 - val_accuracy: 0.6538\n",
            "Epoch 369/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 59.6797 - accuracy: 0.7176 - val_loss: 59.7554 - val_accuracy: 0.6538\n",
            "Epoch 370/500\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 59.6219 - accuracy: 0.7176 - val_loss: 59.6885 - val_accuracy: 0.6538\n",
            "Epoch 371/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 59.5586 - accuracy: 0.7176 - val_loss: 59.6370 - val_accuracy: 0.6538\n",
            "Epoch 372/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 59.5019 - accuracy: 0.7176 - val_loss: 59.5753 - val_accuracy: 0.6538\n",
            "Epoch 373/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 59.4379 - accuracy: 0.7176 - val_loss: 59.5265 - val_accuracy: 0.6538\n",
            "Epoch 374/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 59.3855 - accuracy: 0.7176 - val_loss: 59.4570 - val_accuracy: 0.6538\n",
            "Epoch 375/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 59.3231 - accuracy: 0.7176 - val_loss: 59.4044 - val_accuracy: 0.6538\n",
            "Epoch 376/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 59.2660 - accuracy: 0.7176 - val_loss: 59.3435 - val_accuracy: 0.6538\n",
            "Epoch 377/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 59.2063 - accuracy: 0.7176 - val_loss: 59.2829 - val_accuracy: 0.6538\n",
            "Epoch 378/500\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 59.1465 - accuracy: 0.7176 - val_loss: 59.2235 - val_accuracy: 0.6538\n",
            "Epoch 379/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 59.0935 - accuracy: 0.7176 - val_loss: 59.1639 - val_accuracy: 0.6538\n",
            "Epoch 380/500\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 59.0462 - accuracy: 0.7176 - val_loss: 59.1206 - val_accuracy: 0.6538\n",
            "Epoch 381/500\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 58.9697 - accuracy: 0.7176 - val_loss: 59.0490 - val_accuracy: 0.6538\n",
            "Epoch 382/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 58.9142 - accuracy: 0.7176 - val_loss: 59.0174 - val_accuracy: 0.6538\n",
            "Epoch 383/500\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 58.8643 - accuracy: 0.7176 - val_loss: 58.9321 - val_accuracy: 0.6538\n",
            "Epoch 384/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 58.7977 - accuracy: 0.7176 - val_loss: 58.8732 - val_accuracy: 0.6538\n",
            "Epoch 385/500\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 58.7573 - accuracy: 0.7176 - val_loss: 58.8238 - val_accuracy: 0.6538\n",
            "Epoch 386/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 58.6851 - accuracy: 0.7176 - val_loss: 58.7836 - val_accuracy: 0.6538\n",
            "Epoch 387/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 58.6272 - accuracy: 0.7176 - val_loss: 58.7016 - val_accuracy: 0.6538\n",
            "Epoch 388/500\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 58.5735 - accuracy: 0.7176 - val_loss: 58.6693 - val_accuracy: 0.6538\n",
            "Epoch 389/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 58.5198 - accuracy: 0.7176 - val_loss: 58.6087 - val_accuracy: 0.6538\n",
            "Epoch 390/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 58.4972 - accuracy: 0.7176 - val_loss: 58.5437 - val_accuracy: 0.6538\n",
            "Epoch 391/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 58.4473 - accuracy: 0.7176 - val_loss: 58.4673 - val_accuracy: 0.6538\n",
            "Epoch 392/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 58.3533 - accuracy: 0.7176 - val_loss: 58.4351 - val_accuracy: 0.6538\n",
            "Epoch 393/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 58.3324 - accuracy: 0.7176 - val_loss: 58.3519 - val_accuracy: 0.6538\n",
            "Epoch 394/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 58.2451 - accuracy: 0.7176 - val_loss: 58.2973 - val_accuracy: 0.6538\n",
            "Epoch 395/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 58.1858 - accuracy: 0.7176 - val_loss: 58.2422 - val_accuracy: 0.6538\n",
            "Epoch 396/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 58.1278 - accuracy: 0.7176 - val_loss: 58.1935 - val_accuracy: 0.6538\n",
            "Epoch 397/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 58.0747 - accuracy: 0.7176 - val_loss: 58.1469 - val_accuracy: 0.6538\n",
            "Epoch 398/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 58.0250 - accuracy: 0.7176 - val_loss: 58.0884 - val_accuracy: 0.6538\n",
            "Epoch 399/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 57.9829 - accuracy: 0.7176 - val_loss: 58.0398 - val_accuracy: 0.6538\n",
            "Epoch 400/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 57.8964 - accuracy: 0.7176 - val_loss: 57.9831 - val_accuracy: 0.6538\n",
            "Epoch 401/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 57.8533 - accuracy: 0.7176 - val_loss: 57.9242 - val_accuracy: 0.6538\n",
            "Epoch 402/500\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 57.7967 - accuracy: 0.7176 - val_loss: 57.8667 - val_accuracy: 0.6538\n",
            "Epoch 403/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 57.7365 - accuracy: 0.7176 - val_loss: 57.8139 - val_accuracy: 0.6538\n",
            "Epoch 404/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 57.6869 - accuracy: 0.7176 - val_loss: 57.7598 - val_accuracy: 0.6538\n",
            "Epoch 405/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 57.6213 - accuracy: 0.7176 - val_loss: 57.7051 - val_accuracy: 0.6538\n",
            "Epoch 406/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 57.5666 - accuracy: 0.7176 - val_loss: 57.6619 - val_accuracy: 0.6538\n",
            "Epoch 407/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 57.5163 - accuracy: 0.7176 - val_loss: 57.6048 - val_accuracy: 0.6538\n",
            "Epoch 408/500\n",
            "3/3 [==============================] - 0s 176ms/step - loss: 57.4624 - accuracy: 0.7176 - val_loss: 57.5542 - val_accuracy: 0.6538\n",
            "Epoch 409/500\n",
            "3/3 [==============================] - 0s 178ms/step - loss: 57.3994 - accuracy: 0.7176 - val_loss: 57.4956 - val_accuracy: 0.6538\n",
            "Epoch 410/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 57.3522 - accuracy: 0.7176 - val_loss: 57.4413 - val_accuracy: 0.6538\n",
            "Epoch 411/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 57.2951 - accuracy: 0.7176 - val_loss: 57.3873 - val_accuracy: 0.6538\n",
            "Epoch 412/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 57.2502 - accuracy: 0.7176 - val_loss: 57.3330 - val_accuracy: 0.6538\n",
            "Epoch 413/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 57.1893 - accuracy: 0.7176 - val_loss: 57.2887 - val_accuracy: 0.6538\n",
            "Epoch 414/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 57.1399 - accuracy: 0.7176 - val_loss: 57.2277 - val_accuracy: 0.6538\n",
            "Epoch 415/500\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 57.0771 - accuracy: 0.7176 - val_loss: 57.1844 - val_accuracy: 0.6538\n",
            "Epoch 416/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 57.0272 - accuracy: 0.7176 - val_loss: 57.1183 - val_accuracy: 0.6731\n",
            "Epoch 417/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 56.9779 - accuracy: 0.7176 - val_loss: 57.0658 - val_accuracy: 0.6731\n",
            "Epoch 418/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 56.9220 - accuracy: 0.7176 - val_loss: 57.0125 - val_accuracy: 0.6731\n",
            "Epoch 419/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 56.8611 - accuracy: 0.7176 - val_loss: 56.9711 - val_accuracy: 0.6538\n",
            "Epoch 420/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 56.8101 - accuracy: 0.7176 - val_loss: 56.9080 - val_accuracy: 0.6538\n",
            "Epoch 421/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 56.7584 - accuracy: 0.7176 - val_loss: 56.8880 - val_accuracy: 0.6538\n",
            "Epoch 422/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 56.7293 - accuracy: 0.7176 - val_loss: 56.8214 - val_accuracy: 0.6346\n",
            "Epoch 423/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 56.6797 - accuracy: 0.7176 - val_loss: 56.7645 - val_accuracy: 0.6538\n",
            "Epoch 424/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 56.6105 - accuracy: 0.7176 - val_loss: 56.6936 - val_accuracy: 0.6731\n",
            "Epoch 425/500\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 56.5537 - accuracy: 0.7176 - val_loss: 56.6485 - val_accuracy: 0.6538\n",
            "Epoch 426/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 56.4920 - accuracy: 0.7176 - val_loss: 56.5937 - val_accuracy: 0.6154\n",
            "Epoch 427/500\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 56.4509 - accuracy: 0.7176 - val_loss: 56.5471 - val_accuracy: 0.6154\n",
            "Epoch 428/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 56.3876 - accuracy: 0.7176 - val_loss: 56.5214 - val_accuracy: 0.6538\n",
            "Epoch 429/500\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 56.3487 - accuracy: 0.7176 - val_loss: 56.4501 - val_accuracy: 0.6346\n",
            "Epoch 430/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 56.2928 - accuracy: 0.7176 - val_loss: 56.3941 - val_accuracy: 0.6731\n",
            "Epoch 431/500\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 56.2293 - accuracy: 0.7176 - val_loss: 56.3351 - val_accuracy: 0.6731\n",
            "Epoch 432/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 56.1781 - accuracy: 0.7176 - val_loss: 56.2903 - val_accuracy: 0.6731\n",
            "Epoch 433/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 56.1271 - accuracy: 0.7176 - val_loss: 56.2398 - val_accuracy: 0.6346\n",
            "Epoch 434/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 56.0833 - accuracy: 0.7176 - val_loss: 56.1991 - val_accuracy: 0.6731\n",
            "Epoch 435/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 56.0391 - accuracy: 0.7176 - val_loss: 56.1428 - val_accuracy: 0.6538\n",
            "Epoch 436/500\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 55.9718 - accuracy: 0.7176 - val_loss: 56.0817 - val_accuracy: 0.6154\n",
            "Epoch 437/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 55.9319 - accuracy: 0.7176 - val_loss: 56.0519 - val_accuracy: 0.6731\n",
            "Epoch 438/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 55.8870 - accuracy: 0.7176 - val_loss: 55.9796 - val_accuracy: 0.6346\n",
            "Epoch 439/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 55.8296 - accuracy: 0.7176 - val_loss: 55.9252 - val_accuracy: 0.6731\n",
            "Epoch 440/500\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 55.7711 - accuracy: 0.7176 - val_loss: 55.8832 - val_accuracy: 0.6731\n",
            "Epoch 441/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 55.7294 - accuracy: 0.7176 - val_loss: 55.8281 - val_accuracy: 0.6538\n",
            "Epoch 442/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 55.6720 - accuracy: 0.7176 - val_loss: 55.7813 - val_accuracy: 0.6538\n",
            "Epoch 443/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 55.6257 - accuracy: 0.7176 - val_loss: 55.7325 - val_accuracy: 0.6731\n",
            "Epoch 444/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 55.5762 - accuracy: 0.7176 - val_loss: 55.6803 - val_accuracy: 0.6346\n",
            "Epoch 445/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 55.5164 - accuracy: 0.7235 - val_loss: 55.6367 - val_accuracy: 0.6346\n",
            "Epoch 446/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 55.4687 - accuracy: 0.7176 - val_loss: 55.5900 - val_accuracy: 0.6154\n",
            "Epoch 447/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 55.4141 - accuracy: 0.7176 - val_loss: 55.5348 - val_accuracy: 0.6346\n",
            "Epoch 448/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 55.3609 - accuracy: 0.7176 - val_loss: 55.5191 - val_accuracy: 0.6731\n",
            "Epoch 449/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 55.3189 - accuracy: 0.7176 - val_loss: 55.4444 - val_accuracy: 0.6538\n",
            "Epoch 450/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 55.2712 - accuracy: 0.7353 - val_loss: 55.4185 - val_accuracy: 0.6731\n",
            "Epoch 451/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 55.2347 - accuracy: 0.7235 - val_loss: 55.3364 - val_accuracy: 0.6538\n",
            "Epoch 452/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 55.1669 - accuracy: 0.7235 - val_loss: 55.3243 - val_accuracy: 0.6731\n",
            "Epoch 453/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 55.1210 - accuracy: 0.7176 - val_loss: 55.2442 - val_accuracy: 0.6538\n",
            "Epoch 454/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 55.0779 - accuracy: 0.7353 - val_loss: 55.2097 - val_accuracy: 0.6731\n",
            "Epoch 455/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 55.0236 - accuracy: 0.7176 - val_loss: 55.1390 - val_accuracy: 0.6538\n",
            "Epoch 456/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 54.9840 - accuracy: 0.7471 - val_loss: 55.1534 - val_accuracy: 0.6538\n",
            "Epoch 457/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 54.9590 - accuracy: 0.7294 - val_loss: 55.0615 - val_accuracy: 0.6346\n",
            "Epoch 458/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 54.8976 - accuracy: 0.7235 - val_loss: 54.9941 - val_accuracy: 0.6731\n",
            "Epoch 459/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 54.8484 - accuracy: 0.7235 - val_loss: 54.9310 - val_accuracy: 0.6346\n",
            "Epoch 460/500\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 54.8067 - accuracy: 0.7353 - val_loss: 54.8809 - val_accuracy: 0.6154\n",
            "Epoch 461/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 54.7462 - accuracy: 0.7235 - val_loss: 54.8576 - val_accuracy: 0.6538\n",
            "Epoch 462/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 54.6970 - accuracy: 0.7294 - val_loss: 54.7833 - val_accuracy: 0.6154\n",
            "Epoch 463/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 54.6478 - accuracy: 0.7235 - val_loss: 54.7309 - val_accuracy: 0.6154\n",
            "Epoch 464/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 54.5916 - accuracy: 0.7235 - val_loss: 54.6941 - val_accuracy: 0.6731\n",
            "Epoch 465/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 54.5298 - accuracy: 0.7176 - val_loss: 54.6491 - val_accuracy: 0.6346\n",
            "Epoch 466/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 54.4813 - accuracy: 0.7176 - val_loss: 54.6205 - val_accuracy: 0.6346\n",
            "Epoch 467/500\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 54.4348 - accuracy: 0.7235 - val_loss: 54.5645 - val_accuracy: 0.6346\n",
            "Epoch 468/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 54.3850 - accuracy: 0.7176 - val_loss: 54.5222 - val_accuracy: 0.6346\n",
            "Epoch 469/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 54.3397 - accuracy: 0.7353 - val_loss: 54.5320 - val_accuracy: 0.6731\n",
            "Epoch 470/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 54.3183 - accuracy: 0.7235 - val_loss: 54.4480 - val_accuracy: 0.6346\n",
            "Epoch 471/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 54.2929 - accuracy: 0.7353 - val_loss: 54.3717 - val_accuracy: 0.6731\n",
            "Epoch 472/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 54.2188 - accuracy: 0.7235 - val_loss: 54.3175 - val_accuracy: 0.6731\n",
            "Epoch 473/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 54.1539 - accuracy: 0.7176 - val_loss: 54.2624 - val_accuracy: 0.6346\n",
            "Epoch 474/500\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 54.1156 - accuracy: 0.7176 - val_loss: 54.2095 - val_accuracy: 0.6731\n",
            "Epoch 475/500\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 54.0553 - accuracy: 0.7176 - val_loss: 54.1763 - val_accuracy: 0.6538\n",
            "Epoch 476/500\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 54.0039 - accuracy: 0.7176 - val_loss: 54.1320 - val_accuracy: 0.6538\n",
            "Epoch 477/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 53.9700 - accuracy: 0.7294 - val_loss: 54.1181 - val_accuracy: 0.6154\n",
            "Epoch 478/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 53.9435 - accuracy: 0.7176 - val_loss: 54.0632 - val_accuracy: 0.6538\n",
            "Epoch 479/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 53.9067 - accuracy: 0.7294 - val_loss: 53.9915 - val_accuracy: 0.6731\n",
            "Epoch 480/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 53.8302 - accuracy: 0.7235 - val_loss: 53.9398 - val_accuracy: 0.6731\n",
            "Epoch 481/500\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 53.7758 - accuracy: 0.7294 - val_loss: 53.8949 - val_accuracy: 0.6346\n",
            "Epoch 482/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 53.7277 - accuracy: 0.7235 - val_loss: 53.8647 - val_accuracy: 0.6731\n",
            "Epoch 483/500\n",
            "3/3 [==============================] - 1s 256ms/step - loss: 53.6869 - accuracy: 0.7235 - val_loss: 53.7959 - val_accuracy: 0.6154\n",
            "Epoch 484/500\n",
            "3/3 [==============================] - 0s 126ms/step - loss: 53.6466 - accuracy: 0.7353 - val_loss: 53.7633 - val_accuracy: 0.6538\n",
            "Epoch 485/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 53.5947 - accuracy: 0.7294 - val_loss: 53.7524 - val_accuracy: 0.6731\n",
            "Epoch 486/500\n",
            "3/3 [==============================] - 0s 186ms/step - loss: 53.5574 - accuracy: 0.7176 - val_loss: 53.6894 - val_accuracy: 0.6731\n",
            "Epoch 487/500\n",
            "3/3 [==============================] - 0s 169ms/step - loss: 53.5465 - accuracy: 0.7294 - val_loss: 53.6087 - val_accuracy: 0.6154\n",
            "Epoch 488/500\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 53.5155 - accuracy: 0.7176 - val_loss: 53.5953 - val_accuracy: 0.6538\n",
            "Epoch 489/500\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 53.4458 - accuracy: 0.7294 - val_loss: 53.5408 - val_accuracy: 0.6346\n",
            "Epoch 490/500\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 53.3996 - accuracy: 0.7353 - val_loss: 53.4475 - val_accuracy: 0.6731\n",
            "Epoch 491/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 53.3277 - accuracy: 0.7176 - val_loss: 53.4254 - val_accuracy: 0.6538\n",
            "Epoch 492/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 53.3130 - accuracy: 0.7176 - val_loss: 53.3665 - val_accuracy: 0.6538\n",
            "Epoch 493/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 53.2467 - accuracy: 0.7235 - val_loss: 53.3248 - val_accuracy: 0.6731\n",
            "Epoch 494/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 53.1926 - accuracy: 0.7294 - val_loss: 53.2796 - val_accuracy: 0.6731\n",
            "Epoch 495/500\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 53.1337 - accuracy: 0.7294 - val_loss: 53.2546 - val_accuracy: 0.6731\n",
            "Epoch 496/500\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 53.0911 - accuracy: 0.7294 - val_loss: 53.2019 - val_accuracy: 0.6538\n",
            "Epoch 497/500\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 53.0422 - accuracy: 0.7235 - val_loss: 53.1699 - val_accuracy: 0.6346\n",
            "Epoch 498/500\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 52.9858 - accuracy: 0.7235 - val_loss: 53.1594 - val_accuracy: 0.6731\n",
            "Epoch 499/500\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 52.9612 - accuracy: 0.7176 - val_loss: 53.0895 - val_accuracy: 0.6346\n",
            "Epoch 500/500\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 52.9190 - accuracy: 0.7294 - val_loss: 53.0367 - val_accuracy: 0.6346\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[53.03669738769531, 0.6346153616905212]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "### 8(35) 10(33) 12(SoTA, 1e-3) (8,12)\n",
        "## (42, 0) (12, 100) (12, 12) (42, 100)\n",
        "\n",
        "HeInitializer = tf.keras.initializers.HeNormal()\n",
        "Xavier = tf.keras.initializers.GlorotUniform(\n",
        "    seed=42\n",
        ")\n",
        "RANDOM = 42\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(RANDOM)\n",
        "import random\n",
        "random.seed(RANDOM)\n",
        "import numpy as np\n",
        "np.random.seed(RANDOM)\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    seed(RANDOM)\n",
        "    tf.random.set_seed(RANDOM)\n",
        "    np.random.seed(RANDOM)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.Dense(256, activation=\"relu\", kernel_regularizer='l2', kernel_initializer=HeInitializer)(x)    \n",
        "    x = layers.Dense(128, activation=\"relu\", kernel_regularizer='l2', kernel_initializer=HeInitializer)(x)\n",
        "    x = layers.Dense(inputs.shape[-1], kernel_initializer=Xavier)(x)\n",
        "    return x + res\n",
        "\n",
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "    n_classes = 3\n",
        "):\n",
        "    tf.random.set_seed(RANDOM)\n",
        "    seed(RANDOM)\n",
        "    np.random.seed(RANDOM)\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "\n",
        "    x = layers.Dense(128, activation=\"relu\", kernel_regularizer='l2',kernel_initializer=Xavier)(x)\n",
        "    outputs = layers.Dense(n_classes, activation=\"softmax\",kernel_initializer=Xavier)(x)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "RANDOM = 0\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(RANDOM)\n",
        "import random\n",
        "random.seed(RANDOM)\n",
        "import numpy as np\n",
        "np.random.seed(RANDOM)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "seed(RANDOM)\n",
        "tf.random.set_seed(RANDOM)\n",
        "np.random.seed(RANDOM)\n",
        "model = build_model(\n",
        "    input_shape,\n",
        "    head_size=5,\n",
        "    num_heads=2,\n",
        "    ff_dim=3,\n",
        "    num_transformer_blocks=12,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0,\n",
        "    dropout=0,\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=6e-5),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "history = model.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=1000, restore_best_weights=True)]\n",
        "\n",
        "curve = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=500,\n",
        "    batch_size=64,\n",
        ")\n",
        "\n",
        "model.evaluate(X_test, y_test, verbose=10)    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "S9BQwo-AtzpZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9BQwo-AtzpZ",
        "outputId": "8a921b32-fe7b-492f-8d67-b38c7412d038"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[53.17536544799805, 0.550000011920929]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model.evaluate(X_val, y_val, verbose=10)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "IhK1buKHAGSs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhK1buKHAGSs",
        "outputId": "c4ddcf2b-0e57-48cc-abac-9981beac2660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8431372549019608\n",
            "0.5579831932773109\n",
            "0.5980392156862745\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import roc_curve, confusion_matrix, roc_auc_score\n",
        "import seaborn as sns\n",
        "# import torch.nn.functional as nnf\n",
        "# model.eval()\n",
        "pred = model(X_test)\n",
        "# pred = bestcnn(X_test)\n",
        "pred_prob = tf.nn.softmax(pred, axis=1)\n",
        "\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}      \n",
        "n_class = 3\n",
        "\n",
        "for i in range(n_class):    \n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)\n",
        "\n",
        "a = []\n",
        "classes = [0,1,2]\n",
        "for i in range(len(classes)):\n",
        "    # Gets the class\n",
        "    c = classes[i]\n",
        "    \n",
        "    # Prepares an auxiliar dataframe to help with the plots\n",
        "    df_aux = pd.DataFrame()\n",
        "    df_aux['class'] = [1 if y == c else 0 for y in y_test]\n",
        "    df_aux['prob'] = pred_prob[:, i]\n",
        "    df_aux = df_aux.reset_index(drop = True)\n",
        "\n",
        "    \n",
        "    # Calculates the ROC AUC OvR\n",
        "    a.append(round(roc_auc_score(df_aux['class'], df_aux['prob']),3))\n",
        "    print(roc_auc_score(df_aux['class'], df_aux['prob']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AZwNCYEy1UaM"
      },
      "id": "AZwNCYEy1UaM",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8ml6D_iUGVcJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "8ml6D_iUGVcJ",
        "outputId": "3b5d8a7d-739e-490d-ecd2-03db89111d8c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set_style(\"white\")\n",
        "plt.plot(fpr[0], tpr[0], linestyle='-',color='orange', label=f'class-0 {a[0]}')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='-',color='green', label=f'class-1 {a[1]}')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='-',color='red', label=f'class-2 {a[2]}')\n",
        "# plt.plot(fpr[3], tpr[3], linestyle='-',color='blue', label=f'class-3 {a[3]}')\n",
        "plt.title('Multiclass ROC curve using Transformer',fontsize=15)\n",
        "plt.xlabel('False Positive Rate',fontsize=15)\n",
        "plt.ylabel('True Positive rate',fontsize=15)\n",
        "plt.tick_params(labelsize=15)\n",
        "sns.set(rc={'figure.figsize':(6,6)})\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='grey', label='Random guess')\n",
        "# plt.legend(loc='best',fontsize=10)\n",
        "plt.legend(loc=\"lower right\", fontsize=10);\n",
        "plt.savefig(\"transformer.pdf\",dpi=300);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "WQzWTNLFqDYb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQzWTNLFqDYb",
        "outputId": "e400b9aa-838d-4772-fbaf-5066c9cdb24d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.00      0.00      0.00         1\n",
            "     class 1       0.40      0.12      0.18        17\n",
            "     class 2       0.66      0.91      0.77        34\n",
            "\n",
            "    accuracy                           0.63        52\n",
            "   macro avg       0.35      0.34      0.32        52\n",
            "weighted avg       0.56      0.63      0.56        52\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_true = y_test\n",
        "y_pred = np.argmax(model(X_test), axis=1)\n",
        "target_names = ['class 0', 'class 1', 'class 2']\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f6i6ri_ckLiJ",
      "metadata": {
        "id": "f6i6ri_ckLiJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "d0e5e194-a24b-4299-d5f9-39d6d75fad66"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    122\u001b[0m       'TBE_EPHEM_CREDS_ADDR'] if ephemeral else _os.environ['TBE_CREDS_ADDR']\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    125\u001b[0m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IjhQzALexdxV",
      "metadata": {
        "id": "IjhQzALexdxV"
      },
      "outputs": [],
      "source": [
        "# bestmodel = keras.models.load_model(\"drive/MyDrive/model_transformer_final\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e80531b8",
      "metadata": {
        "id": "e80531b8"
      },
      "outputs": [],
      "source": [
        "def extract(arr):\n",
        "  out = []\n",
        "  for weights in arr:\n",
        "    max_weights = np.argmax(weights)\n",
        "    out.append(max_weights)\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e783ouAv1EPM",
      "metadata": {
        "id": "e783ouAv1EPM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "sns.set_style(\"white\")\n",
        "plt.plot(curve.history['loss'][0:1000])\n",
        "plt.plot(curve.history['val_loss'][0:1000])\n",
        "plt.title(\"Loss Curve of Transformer-based Model\",fontsize=15)\n",
        "plt.ylabel('loss',fontsize=15)\n",
        "plt.xlabel('epoch',fontsize=15)\n",
        "plt.tick_params(labelsize=15)\n",
        "sns.set(rc={'figure.figsize':(6,6)})\n",
        "# plt.ylim(0.8, 2.2)\n",
        "plt.legend(['train', \"validation\"], loc='upper right')\n",
        "plt.savefig(\"transformer_loss.pdf\",dpi=300);\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0_QFfeHhOBuw",
      "metadata": {
        "id": "0_QFfeHhOBuw"
      },
      "outputs": [],
      "source": [
        "plt.plot(curve.history['accuracy'])\n",
        "plt.plot(curve.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(curve.history['loss'][:500])\n",
        "plt.plot(curve.history['val_loss'][:500])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81416fc8",
      "metadata": {
        "id": "81416fc8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9b5193a6",
        "OMKsNJVxJZMF",
        "842c0fc9",
        "e023e5aa"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "3f2099620ee754a799ea2efde844907f572fbdcd9ece8dce41bdce43c5c037bd"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}